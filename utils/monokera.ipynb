{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>email</th>\n",
       "      <th>gender</th>\n",
       "      <th>policy_number</th>\n",
       "      <th>insured_name</th>\n",
       "      <th>insured_gender</th>\n",
       "      <th>insured_age</th>\n",
       "      <th>insured_address</th>\n",
       "      <th>...</th>\n",
       "      <th>agent_email</th>\n",
       "      <th>agent_phone</th>\n",
       "      <th>claim_status</th>\n",
       "      <th>claim_date</th>\n",
       "      <th>claim_amount</th>\n",
       "      <th>claim_description</th>\n",
       "      <th>payment_status</th>\n",
       "      <th>payment_date</th>\n",
       "      <th>payment_amount</th>\n",
       "      <th>payment_method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Bessy</td>\n",
       "      <td>Tomaszynski</td>\n",
       "      <td>btomaszynski0@icio.us</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>Bessy Tomaszynski</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1 Northland Trail</td>\n",
       "      <td>...</td>\n",
       "      <td>btomaszynski0@alexa.com</td>\n",
       "      <td>+1 (757) 646-3536</td>\n",
       "      <td>Approved</td>\n",
       "      <td>12/11/2022</td>\n",
       "      <td>7589.60</td>\n",
       "      <td>Duis bibendum, felis sed interdum venenatis, t...</td>\n",
       "      <td>Overdue</td>\n",
       "      <td>3/19/2022</td>\n",
       "      <td>644.58</td>\n",
       "      <td>Credit Card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Cher</td>\n",
       "      <td>Gjerde</td>\n",
       "      <td>cgjerde1@hexun.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Cher Gjerde</td>\n",
       "      <td>Female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>98989 Basil Way</td>\n",
       "      <td>...</td>\n",
       "      <td>cgjerde1@fotki.com</td>\n",
       "      <td>+1 (850) 302-7531</td>\n",
       "      <td>Denied</td>\n",
       "      <td>8/20/2022</td>\n",
       "      <td>5777.00</td>\n",
       "      <td>Quisque id justo sit amet sapien dignissim ves...</td>\n",
       "      <td>Overdue</td>\n",
       "      <td>6/3/2022</td>\n",
       "      <td>912.79</td>\n",
       "      <td>Check</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Felicity</td>\n",
       "      <td>Lowman</td>\n",
       "      <td>flowman2@flavors.me</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "      <td>Felicity Lowman</td>\n",
       "      <td>Female</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2 Derek Place</td>\n",
       "      <td>...</td>\n",
       "      <td>flowman2@wikispaces.com</td>\n",
       "      <td>+1 (754) 879-3501</td>\n",
       "      <td>Pending</td>\n",
       "      <td>5/4/2022</td>\n",
       "      <td>1684.43</td>\n",
       "      <td>In sagittis dui vel nisl. Duis ac nibh. Fusce ...</td>\n",
       "      <td>Pending</td>\n",
       "      <td>8/23/2022</td>\n",
       "      <td>995.75</td>\n",
       "      <td>Check</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Ive</td>\n",
       "      <td>Jervois</td>\n",
       "      <td>ijervois3@epa.gov</td>\n",
       "      <td>Male</td>\n",
       "      <td>4</td>\n",
       "      <td>Ive Jervois</td>\n",
       "      <td>Male</td>\n",
       "      <td>60.0</td>\n",
       "      <td>889 Twin Pines Court</td>\n",
       "      <td>...</td>\n",
       "      <td>ijervois3@ebay.co.uk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Approved</td>\n",
       "      <td>8/9/2022</td>\n",
       "      <td>8160.13</td>\n",
       "      <td>In hac habitasse platea dictumst. Etiam faucib...</td>\n",
       "      <td>Pending</td>\n",
       "      <td>11/18/2022</td>\n",
       "      <td>190.06</td>\n",
       "      <td>Bank Transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Gretal</td>\n",
       "      <td>Bauckham</td>\n",
       "      <td>gbauckham4@slideshare.net</td>\n",
       "      <td>Female</td>\n",
       "      <td>5</td>\n",
       "      <td>Gretal Bauckham</td>\n",
       "      <td>Female</td>\n",
       "      <td>29.0</td>\n",
       "      <td>39745 Clarendon Hill</td>\n",
       "      <td>...</td>\n",
       "      <td>gbauckham4@goo.gl</td>\n",
       "      <td>+1 (480) 648-4238</td>\n",
       "      <td>Approved</td>\n",
       "      <td>6/8/2022</td>\n",
       "      <td>9694.67</td>\n",
       "      <td>Praesent id massa id nisl venenatis lacinia. A...</td>\n",
       "      <td>Pending</td>\n",
       "      <td>11/26/2022</td>\n",
       "      <td>645.52</td>\n",
       "      <td>Check</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id first_name    last_name                      email  gender  \\\n",
       "0  1.0      Bessy  Tomaszynski      btomaszynski0@icio.us    Male   \n",
       "1  2.0       Cher       Gjerde         cgjerde1@hexun.com     NaN   \n",
       "2  3.0   Felicity       Lowman        flowman2@flavors.me    Male   \n",
       "3  4.0        Ive      Jervois          ijervois3@epa.gov    Male   \n",
       "4  5.0     Gretal     Bauckham  gbauckham4@slideshare.net  Female   \n",
       "\n",
       "   policy_number       insured_name insured_gender  insured_age  \\\n",
       "0              1  Bessy Tomaszynski            NaN         63.0   \n",
       "1              2        Cher Gjerde         Female         19.0   \n",
       "2              3    Felicity Lowman         Female         43.0   \n",
       "3              4        Ive Jervois           Male         60.0   \n",
       "4              5    Gretal Bauckham         Female         29.0   \n",
       "\n",
       "        insured_address  ...              agent_email        agent_phone  \\\n",
       "0     1 Northland Trail  ...  btomaszynski0@alexa.com  +1 (757) 646-3536   \n",
       "1       98989 Basil Way  ...       cgjerde1@fotki.com  +1 (850) 302-7531   \n",
       "2         2 Derek Place  ...  flowman2@wikispaces.com  +1 (754) 879-3501   \n",
       "3  889 Twin Pines Court  ...     ijervois3@ebay.co.uk                NaN   \n",
       "4  39745 Clarendon Hill  ...        gbauckham4@goo.gl  +1 (480) 648-4238   \n",
       "\n",
       "   claim_status  claim_date claim_amount  \\\n",
       "0      Approved  12/11/2022      7589.60   \n",
       "1        Denied   8/20/2022      5777.00   \n",
       "2       Pending    5/4/2022      1684.43   \n",
       "3      Approved    8/9/2022      8160.13   \n",
       "4      Approved    6/8/2022      9694.67   \n",
       "\n",
       "                                   claim_description  payment_status  \\\n",
       "0  Duis bibendum, felis sed interdum venenatis, t...         Overdue   \n",
       "1  Quisque id justo sit amet sapien dignissim ves...         Overdue   \n",
       "2  In sagittis dui vel nisl. Duis ac nibh. Fusce ...         Pending   \n",
       "3  In hac habitasse platea dictumst. Etiam faucib...         Pending   \n",
       "4  Praesent id massa id nisl venenatis lacinia. A...         Pending   \n",
       "\n",
       "   payment_date  payment_amount payment_method  \n",
       "0     3/19/2022          644.58    Credit Card  \n",
       "1      6/3/2022          912.79          Check  \n",
       "2     8/23/2022          995.75          Check  \n",
       "3    11/18/2022          190.06  Bank Transfer  \n",
       "4    11/26/2022          645.52          Check  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta del archivo cargado\n",
    "file_path = \"C:\\\\Users\\\\Usuario\\\\Documents\\\\WILLIAM\\\\DOCUMENTOS\\\\PERSONALES\\\\MONOKERA\\\\MOCK_DATA.csv\"\n",
    "\n",
    "# Cargar el archivo para inspeccionar su contenido\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Mostrar las primeras filas para análisis inicial\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+-----------+-------------+-----------------+--------------+-----------+-----------------+------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+-----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "|                  id|          first_name|           last_name|               email|     gender|policy_number|     insured_name|insured_gender|insured_age|  insured_address|insured_city|insured_state|insured_postal_code|insured_country|policy_start_date|policy_end_date|premium_amount|deductible_amount|coverage_limit|policy_type|insurance_company|       agent_name|         agent_email|      agent_phone|claim_status|claim_date|claim_amount|   claim_description|payment_status|payment_date|payment_amount|payment_method|\n",
      "+--------------------+--------------------+--------------------+--------------------+-----------+-------------+-----------------+--------------+-----------+-----------------+------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+-----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "|                   1|               Bessy|         Tomaszynski|btomaszynski0@ici...|       Male|            1|Bessy Tomaszynski|   Desconocido|         63|1 Northland Trail|     Suffolk|     Virginia|               NULL|           NULL|       2023-06-05|     03/13/2023|       7506.95|           263.51|     851969.14|       Home|            Jazzy|Bessy Tomaszynski|btomaszynski0@ale...|+1 (757) 646-3536|    Approved|2022-12-11|      7589.6|Duis bibendum, fe...|       Unknown|        NULL|          NULL|       Unknown|\n",
      "|In hac habitasse ...| velit id pretium...| diam erat fermen...| nec condimentum ...|       NULL|         NULL|             NULL|   Desconocido|          0|             NULL|        NULL|         NULL|               NULL|           NULL|             NULL|           NULL|          NULL|             NULL|          NULL|       NULL|             NULL|             NULL|                NULL|             NULL|     Unknown|      NULL|        NULL|                NULL|       Unknown|        NULL|          NULL|       Unknown|\n",
      "|Aliquam quis turp...|             Overdue|           3/19/2022|              644.58|Credit Card|         NULL|             NULL|   Desconocido|          0|             NULL|        NULL|         NULL|               NULL|           NULL|             NULL|           NULL|          NULL|             NULL|          NULL|       NULL|             NULL|             NULL|                NULL|             NULL|     Unknown|      NULL|        NULL|                NULL|       Unknown|        NULL|          NULL|       Unknown|\n",
      "|                   2|                Cher|              Gjerde|  cgjerde1@hexun.com|       NULL|            2|      Cher Gjerde|        Female|         19|  98989 Basil Way|   Pensacola|      Florida|              32526|           NULL|       2023-11-09|     01/24/2023|       1856.49|          4149.44|     138025.78|       Life|      Brainlounge|      Cher Gjerde|  cgjerde1@fotki.com|+1 (850) 302-7531|      Denied|2022-08-20|      5777.0|Quisque id justo ...|       Unknown|        NULL|          NULL|       Unknown|\n",
      "|Vestibulum ac est...| diam id ornare i...| sapien urna pret...| ut volutpat sapi...|    Overdue|     6/3/2022|           912.79|         Check|          0|             NULL|        NULL|         NULL|               NULL|           NULL|             NULL|           NULL|          NULL|             NULL|          NULL|       NULL|             NULL|             NULL|                NULL|             NULL|     Unknown|      NULL|        NULL|                NULL|       Unknown|        NULL|          NULL|       Unknown|\n",
      "+--------------------+--------------------+--------------------+--------------------+-----------+-------------+-----------------+--------------+-----------+-----------------+------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+-----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "ETL completado y datos guardados en PostgreSQL.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Crear la sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"ETL_Policy_Data\").getOrCreate()\n",
    "\n",
    "# Ruta del archivo\n",
    "file_path = \"C:\\\\Users\\\\Usuario\\\\Documents\\\\WILLIAM\\\\DOCUMENTOS\\\\PERSONALES\\\\MONOKERA\\\\MOCK_DATA.csv\"\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Limpieza de datos\n",
    "# Convertir fechas al formato correcto\n",
    "date_columns = [\"claim_date\", \"payment_date\"]\n",
    "for col_name in date_columns:\n",
    "    df = df.withColumn(col_name, to_date(col(col_name), \"M/d/yyyy\"))\n",
    "\n",
    "# Manejo de valores nulos\n",
    "df_cleaned = df.fillna({\n",
    "    \"insured_age\": 0,\n",
    "    \"insured_gender\": \"Desconocido\",\n",
    "    \"claim_status\": \"Unknown\",\n",
    "    \"payment_status\": \"Unknown\",\n",
    "    \"payment_method\": \"Unknown\"\n",
    "})\n",
    "\n",
    "# Convertir columnas numéricas relevantes a tipos adecuados\n",
    "numeric_columns = [\"claim_amount\", \"payment_amount\"]\n",
    "for col_name in numeric_columns:\n",
    "    df_cleaned = df_cleaned.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "\n",
    "# Mostrar las primeras filas después de limpieza\n",
    "df_cleaned.show(5)\n",
    "\n",
    "print(\"ETL completado y datos guardados en PostgreSQL.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id first_name    last_name                      email  gender  \\\n",
      "0  1.0      Bessy  Tomaszynski      btomaszynski0@icio.us    Male   \n",
      "1  2.0       Cher       Gjerde         cgjerde1@hexun.com     NaN   \n",
      "2  3.0   Felicity       Lowman        flowman2@flavors.me    Male   \n",
      "3  4.0        Ive      Jervois          ijervois3@epa.gov    Male   \n",
      "4  5.0     Gretal     Bauckham  gbauckham4@slideshare.net  Female   \n",
      "\n",
      "   policy_number       insured_name insured_gender  insured_age  \\\n",
      "0              1  Bessy Tomaszynski    Desconocido         63.0   \n",
      "1              2        Cher Gjerde         Female         19.0   \n",
      "2              3    Felicity Lowman         Female         43.0   \n",
      "3              4        Ive Jervois           Male         60.0   \n",
      "4              5    Gretal Bauckham         Female         29.0   \n",
      "\n",
      "        insured_address  ...              agent_email        agent_phone  \\\n",
      "0     1 Northland Trail  ...  btomaszynski0@alexa.com  +1 (757) 646-3536   \n",
      "1       98989 Basil Way  ...       cgjerde1@fotki.com  +1 (850) 302-7531   \n",
      "2         2 Derek Place  ...  flowman2@wikispaces.com  +1 (754) 879-3501   \n",
      "3  889 Twin Pines Court  ...     ijervois3@ebay.co.uk                NaN   \n",
      "4  39745 Clarendon Hill  ...        gbauckham4@goo.gl  +1 (480) 648-4238   \n",
      "\n",
      "   claim_status claim_date claim_amount  \\\n",
      "0      Approved 2022-12-11      7589.60   \n",
      "1        Denied 2022-08-20      5777.00   \n",
      "2       Pending 2022-05-04      1684.43   \n",
      "3      Approved 2022-08-09      8160.13   \n",
      "4      Approved 2022-06-08      9694.67   \n",
      "\n",
      "                                   claim_description  payment_status  \\\n",
      "0  Duis bibendum, felis sed interdum venenatis, t...         Overdue   \n",
      "1  Quisque id justo sit amet sapien dignissim ves...         Overdue   \n",
      "2  In sagittis dui vel nisl. Duis ac nibh. Fusce ...         Pending   \n",
      "3  In hac habitasse platea dictumst. Etiam faucib...         Pending   \n",
      "4  Praesent id massa id nisl venenatis lacinia. A...         Pending   \n",
      "\n",
      "   payment_date  payment_amount payment_method  \n",
      "0    2022-03-19          644.58    Credit Card  \n",
      "1    2022-06-03          912.79          Check  \n",
      "2    2022-08-23          995.75          Check  \n",
      "3    2022-11-18          190.06  Bank Transfer  \n",
      "4    2022-11-26          645.52          Check  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta del archivo cargado\n",
    "file_path = \"C:\\\\Users\\\\Usuario\\\\Documents\\\\WILLIAM\\\\DOCUMENTOS\\\\PERSONALES\\\\MONOKERA\\\\MOCK_DATA.csv\"\n",
    "\n",
    "# Cargar el archivo para inspeccionar su contenido\n",
    "data_2 = pd.read_csv(file_path)\n",
    "\n",
    "# Convertir fechas al formato correcto\n",
    "date_columns = [\"claim_date\", \"payment_date\"]\n",
    "for col_name in date_columns:\n",
    "    data_2[col_name] = pd.to_datetime(data_2[col_name], format=\"%m/%d/%Y\", errors=\"coerce\")\n",
    "\n",
    "# Manejo de valores nulos\n",
    "data_2.fillna({\n",
    "    \"insured_age\": 0,\n",
    "    \"insured_gender\": \"Desconocido\",\n",
    "    \"claim_status\": \"Unknown\",\n",
    "    \"payment_status\": \"Unknown\",\n",
    "    \"payment_method\": \"Unknown\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Convertir columnas numéricas relevantes a tipos adecuados\n",
    "numeric_columns = [\"claim_amount\", \"payment_amount\"]\n",
    "for col_name in numeric_columns:\n",
    "    data_2[col_name] = pd.to_numeric(data_2[col_name], errors=\"coerce\")\n",
    "\n",
    "# Mostrar las primeras filas después de limpieza\n",
    "print(data_2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:  ['id', 'first_name', 'last_name', 'email', 'gender', 'policy_number', 'insured_name', 'insured_gender', 'insured_age', 'insured_address', 'insured_city', 'insured_state', 'insured_postal_code', 'insured_country', 'policy_start_date', 'policy_end_date', 'premium_amount', 'deductible_amount', 'coverage_limit', 'policy_type', 'insurance_company', 'agent_name', 'agent_email', 'agent_phone', 'claim_status', 'claim_date', 'claim_amount', 'claim_description', 'payment_status', 'payment_date', 'payment_amount', 'payment_method']\n",
      "+---+----------+-----------+--------------------+------+-------------+-----------------+--------------+-----------+--------------------+-------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+-----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "| id|first_name|  last_name|               email|gender|policy_number|     insured_name|insured_gender|insured_age|     insured_address| insured_city|insured_state|insured_postal_code|insured_country|policy_start_date|policy_end_date|premium_amount|deductible_amount|coverage_limit|policy_type|insurance_company|       agent_name|         agent_email|      agent_phone|claim_status|claim_date|claim_amount|   claim_description|payment_status|payment_date|payment_amount|payment_method|\n",
      "+---+----------+-----------+--------------------+------+-------------+-----------------+--------------+-----------+--------------------+-------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+-----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "|  1|     Bessy|Tomaszynski|btomaszynski0@ici...|  Male|            1|Bessy Tomaszynski|       Unknown|         63|   1 Northland Trail|      Suffolk|     Virginia|               NULL|           NULL|       2023-06-05|     03/13/2023|       7506.95|           263.51|     851969.14|       Home|            Jazzy|Bessy Tomaszynski|btomaszynski0@ale...|+1 (757) 646-3536|    Approved|2022-12-11|      7589.6|Duis bibendum, fe...|       Unknown|        NULL|          NULL|       Unknown|\n",
      "|  2|      Cher|     Gjerde|  cgjerde1@hexun.com|  NULL|            2|      Cher Gjerde|        Female|         19|     98989 Basil Way|    Pensacola|      Florida|              32526|           NULL|       2023-11-09|     01/24/2023|       1856.49|          4149.44|     138025.78|       Life|      Brainlounge|      Cher Gjerde|  cgjerde1@fotki.com|+1 (850) 302-7531|      Denied|2022-08-20|      5777.0|Quisque id justo ...|       Unknown|        NULL|          NULL|       Unknown|\n",
      "|  3|  Felicity|     Lowman| flowman2@flavors.me|  Male|            3|  Felicity Lowman|        Female|         43|       2 Derek Place|Pompano Beach|      Florida|              33075|           NULL|       2023-07-21|     05/05/2023|       9799.03|           598.63|     219401.29|       Life|         Realfire|  Felicity Lowman|flowman2@wikispac...|+1 (754) 879-3501|     Pending|2022-05-04|     1684.43|In sagittis dui v...|       Unknown|        NULL|          NULL|       Unknown|\n",
      "|  4|       Ive|    Jervois|   ijervois3@epa.gov|  Male|            4|      Ive Jervois|          Male|         60|889 Twin Pines Court|       Albany|     New York|              12242|  United States|       2023-08-08|     07/18/2023|        290.53|          4602.64|     669242.87|       Life|      Babblestorm|      Ive Jervois|ijervois3@ebay.co.uk|             NULL|    Approved|2022-08-09|     8160.13|In hac habitasse ...|       Unknown|        NULL|          NULL|       Unknown|\n",
      "|  5|    Gretal|   Bauckham|gbauckham4@slides...|Female|            5|  Gretal Bauckham|        Female|         29|39745 Clarendon Hill|      Phoenix|      Arizona|               NULL|  United States|       2023-12-24|     03/07/2023|        307.87|          1618.01|     647041.09|       Home|          Tagtune|  Gretal Bauckham|   gbauckham4@goo.gl|+1 (480) 648-4238|    Approved|2022-06-08|     9694.67|Praesent id massa...|       Unknown|        NULL|          NULL|       Unknown|\n",
      "+---+----------+-----------+--------------------+------+-------------+-----------------+--------------+-----------+--------------------+-------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+-----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 32 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   id                   932 non-null    float64\n",
      " 1   first_name           1000 non-null   object \n",
      " 2   last_name            883 non-null    object \n",
      " 3   email                935 non-null    object \n",
      " 4   gender               870 non-null    object \n",
      " 5   policy_number        1000 non-null   int64  \n",
      " 6   insured_name         1000 non-null   object \n",
      " 7   insured_gender       936 non-null    object \n",
      " 8   insured_age          938 non-null    float64\n",
      " 9   insured_address      881 non-null    object \n",
      " 10  insured_city         783 non-null    object \n",
      " 11  insured_state        1000 non-null   object \n",
      " 12  insured_postal_code  788 non-null    float64\n",
      " 13  insured_country      679 non-null    object \n",
      " 14  policy_start_date    1000 non-null   object \n",
      " 15  policy_end_date      1000 non-null   object \n",
      " 16  premium_amount       1000 non-null   float64\n",
      " 17  deductible_amount    1000 non-null   float64\n",
      " 18  coverage_limit       945 non-null    float64\n",
      " 19  policy_type          1000 non-null   object \n",
      " 20  insurance_company    1000 non-null   object \n",
      " 21  agent_name           1000 non-null   object \n",
      " 22  agent_email          1000 non-null   object \n",
      " 23  agent_phone          889 non-null    object \n",
      " 24  claim_status         1000 non-null   object \n",
      " 25  claim_date           1000 non-null   object \n",
      " 26  claim_amount         1000 non-null   float64\n",
      " 27  claim_description    1000 non-null   object \n",
      " 28  payment_status       1000 non-null   object \n",
      " 29  payment_date         1000 non-null   object \n",
      " 30  payment_amount       1000 non-null   float64\n",
      " 31  payment_method       1000 non-null   object \n",
      "dtypes: float64(8), int64(1), object(23)\n",
      "memory usage: 250.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(    id first_name    last_name                      email  gender  \\\n",
       " 0  1.0      Bessy  Tomaszynski      btomaszynski0@icio.us    Male   \n",
       " 1  2.0       Cher       Gjerde         cgjerde1@hexun.com     NaN   \n",
       " 2  3.0   Felicity       Lowman        flowman2@flavors.me    Male   \n",
       " 3  4.0        Ive      Jervois          ijervois3@epa.gov    Male   \n",
       " 4  5.0     Gretal     Bauckham  gbauckham4@slideshare.net  Female   \n",
       " \n",
       "    policy_number       insured_name insured_gender  insured_age  \\\n",
       " 0              1  Bessy Tomaszynski            NaN         63.0   \n",
       " 1              2        Cher Gjerde         Female         19.0   \n",
       " 2              3    Felicity Lowman         Female         43.0   \n",
       " 3              4        Ive Jervois           Male         60.0   \n",
       " 4              5    Gretal Bauckham         Female         29.0   \n",
       " \n",
       "         insured_address  ...              agent_email        agent_phone  \\\n",
       " 0     1 Northland Trail  ...  btomaszynski0@alexa.com  +1 (757) 646-3536   \n",
       " 1       98989 Basil Way  ...       cgjerde1@fotki.com  +1 (850) 302-7531   \n",
       " 2         2 Derek Place  ...  flowman2@wikispaces.com  +1 (754) 879-3501   \n",
       " 3  889 Twin Pines Court  ...     ijervois3@ebay.co.uk                NaN   \n",
       " 4  39745 Clarendon Hill  ...        gbauckham4@goo.gl  +1 (480) 648-4238   \n",
       " \n",
       "    claim_status  claim_date claim_amount  \\\n",
       " 0      Approved  12/11/2022      7589.60   \n",
       " 1        Denied   8/20/2022      5777.00   \n",
       " 2       Pending    5/4/2022      1684.43   \n",
       " 3      Approved    8/9/2022      8160.13   \n",
       " 4      Approved    6/8/2022      9694.67   \n",
       " \n",
       "                                    claim_description  payment_status  \\\n",
       " 0  Duis bibendum, felis sed interdum venenatis, t...         Overdue   \n",
       " 1  Quisque id justo sit amet sapien dignissim ves...         Overdue   \n",
       " 2  In sagittis dui vel nisl. Duis ac nibh. Fusce ...         Pending   \n",
       " 3  In hac habitasse platea dictumst. Etiam faucib...         Pending   \n",
       " 4  Praesent id massa id nisl venenatis lacinia. A...         Pending   \n",
       " \n",
       "    payment_date  payment_amount payment_method  \n",
       " 0     3/19/2022          644.58    Credit Card  \n",
       " 1      6/3/2022          912.79          Check  \n",
       " 2     8/23/2022          995.75          Check  \n",
       " 3    11/18/2022          190.06  Bank Transfer  \n",
       " 4    11/26/2022          645.52          Check  \n",
       " \n",
       " [5 rows x 32 columns],\n",
       " None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, regexp_extract\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Inicializa la sesión de Spark\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"DataCleaning\").getOrCreate()\n",
    "\n",
    "# Ruta del archivo cargado\n",
    "file_path = \"C:\\\\Users\\\\Usuario\\\\Documents\\\\WILLIAM\\\\DOCUMENTOS\\\\PERSONALES\\\\MONOKERA\\\\MOCK_DATA.csv\"\n",
    "\n",
    "# Leer el archivo CSV y asegurarse de que tiene encabezado y tipo de datos inferido\n",
    "data_3 = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Verificar los nombres de las columnas\n",
    "print(\"Column names: \", data_3.columns)\n",
    "\n",
    "# Limpiar la columna 'id' para extraer solo los números (si están en el formato esperado)\n",
    "# Usamos expresiones regulares para extraer solo la parte numérica\n",
    "data_3 = data_3.withColumn('id', regexp_extract(col('id'), r'\\d+', 0))\n",
    "\n",
    "# Filtrar filas donde 'id' no es nulo y contiene solo números\n",
    "data_3_cleaned = data_3.filter(col('id').rlike('^\\d+$'))\n",
    "\n",
    "# Verificar si las columnas de fecha existen y convertirlas al formato correcto\n",
    "date_columns = [\"claim_date\", \"payment_date\"]\n",
    "for col_name in date_columns:\n",
    "    if col_name in data_3_cleaned.columns:\n",
    "        data_3_cleaned = data_3_cleaned.withColumn(col_name, to_date(col(col_name), \"M/d/yyyy\"))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada\")\n",
    "\n",
    "# Manejo de valores nulos\n",
    "# Rellenar valores nulos con valores predeterminados\n",
    "data_cleaned = data_3_cleaned.fillna({\n",
    "    \"insured_age\": 0,\n",
    "    \"insured_gender\": \"Unknown\",\n",
    "    \"claim_status\": \"Unknown\",\n",
    "    \"payment_status\": \"Unknown\",\n",
    "    \"payment_method\": \"Unknown\"\n",
    "})\n",
    "\n",
    "# Convertir columnas numéricas relevantes a tipos adecuados\n",
    "numeric_columns = [\"claim_amount\", \"payment_amount\"]\n",
    "for col_name in numeric_columns:\n",
    "    if col_name in data_cleaned.columns:\n",
    "        data_cleaned = data_cleaned.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada para conversión a tipo numérico\")\n",
    "\n",
    "# Mostrar las primeras filas después de limpieza\n",
    "data_cleaned.show(5)\n",
    "\n",
    "#data_filtered = data_cleaned.filter((col('payment_amount') > 0) & (col('payment_amount').isNotNull()))\n",
    "\n",
    "#data_filtered.show(5)\n",
    "#data.head(), data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:  ['id', 'first_name', 'last_name', 'email', 'gender', 'policy_number', 'insured_name', 'insured_gender', 'insured_age', 'insured_address', 'insured_city', 'insured_state', 'insured_postal_code', 'insured_country', 'policy_start_date', 'policy_end_date', 'premium_amount', 'deductible_amount', 'coverage_limit', 'policy_type', 'insurance_company', 'agent_name', 'agent_email', 'agent_phone', 'claim_status', 'claim_date', 'claim_amount', 'claim_description', 'payment_status', 'payment_date', 'payment_amount', 'payment_method']\n",
      "+---+----------+-----------+--------------------+------+-------------+-----------------+--------------+-----------+--------------------+-------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+-----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "| id|first_name|  last_name|               email|gender|policy_number|     insured_name|insured_gender|insured_age|     insured_address| insured_city|insured_state|insured_postal_code|insured_country|policy_start_date|policy_end_date|premium_amount|deductible_amount|coverage_limit|policy_type|insurance_company|       agent_name|         agent_email|      agent_phone|claim_status|claim_date|claim_amount|   claim_description|payment_status|payment_date|payment_amount|payment_method|\n",
      "+---+----------+-----------+--------------------+------+-------------+-----------------+--------------+-----------+--------------------+-------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+-----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "|  1|     Bessy|Tomaszynski|btomaszynski0@ici...|  Male|            1|Bessy Tomaszynski|       Unknown|         63|   1 Northland Trail|      Suffolk|     Virginia|               NULL|           NULL|       2023-06-05|     03/13/2023|       7506.95|           263.51|     851969.14|       Home|            Jazzy|Bessy Tomaszynski|btomaszynski0@ale...|+1 (757) 646-3536|    Approved|2022-12-11|      7589.6|Duis bibendum, fe...|       Unknown|        NULL|          NULL|       Unknown|\n",
      "|  2|      Cher|     Gjerde|  cgjerde1@hexun.com|  NULL|            2|      Cher Gjerde|        Female|         19|     98989 Basil Way|    Pensacola|      Florida|              32526|           NULL|       2023-11-09|     01/24/2023|       1856.49|          4149.44|     138025.78|       Life|      Brainlounge|      Cher Gjerde|  cgjerde1@fotki.com|+1 (850) 302-7531|      Denied|2022-08-20|      5777.0|Quisque id justo ...|       Unknown|        NULL|          NULL|       Unknown|\n",
      "|  3|  Felicity|     Lowman| flowman2@flavors.me|  Male|            3|  Felicity Lowman|        Female|         43|       2 Derek Place|Pompano Beach|      Florida|              33075|           NULL|       2023-07-21|     05/05/2023|       9799.03|           598.63|     219401.29|       Life|         Realfire|  Felicity Lowman|flowman2@wikispac...|+1 (754) 879-3501|     Pending|2022-05-04|     1684.43|In sagittis dui v...|       Unknown|        NULL|          NULL|       Unknown|\n",
      "|  4|       Ive|    Jervois|   ijervois3@epa.gov|  Male|            4|      Ive Jervois|          Male|         60|889 Twin Pines Court|       Albany|     New York|              12242|  United States|       2023-08-08|     07/18/2023|        290.53|          4602.64|     669242.87|       Life|      Babblestorm|      Ive Jervois|ijervois3@ebay.co.uk|             NULL|    Approved|2022-08-09|     8160.13|In hac habitasse ...|       Unknown|        NULL|          NULL|       Unknown|\n",
      "|  5|    Gretal|   Bauckham|gbauckham4@slides...|Female|            5|  Gretal Bauckham|        Female|         29|39745 Clarendon Hill|      Phoenix|      Arizona|               NULL|  United States|       2023-12-24|     03/07/2023|        307.87|          1618.01|     647041.09|       Home|          Tagtune|  Gretal Bauckham|   gbauckham4@goo.gl|+1 (480) 648-4238|    Approved|2022-06-08|     9694.67|Praesent id massa...|       Unknown|        NULL|          NULL|       Unknown|\n",
      "+---+----------+-----------+--------------------+------+-------------+-----------------+--------------+-----------+--------------------+-------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+-----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o915.jdbc.\n: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\r\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:766)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# **1. Insertar en la tabla 'Policy'**\u001b[39;00m\n\u001b[0;32m     65\u001b[0m policy_data \u001b[38;5;241m=\u001b[39m data_cleaned\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_number\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_start_date\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_end_date\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsurance_company\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 66\u001b[0m \u001b[43mpolicy_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjdbc_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPolicy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproperties\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# **2. Insertar en la tabla 'Insured'**\u001b[39;00m\n\u001b[0;32m     69\u001b[0m insured_data \u001b[38;5;241m=\u001b[39m data_cleaned\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_number\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_gender\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_age\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     70\u001b[0m                                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_address\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_city\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_postal_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_country\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\readwriter.py:1984\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[1;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[0;32m   1982\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[0;32m   1983\u001b[0m     jprop\u001b[38;5;241m.\u001b[39msetProperty(k, properties[k])\n\u001b[1;32m-> 1984\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o915.jdbc.\n: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\r\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:766)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, regexp_extract\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Inicializa la sesión de Spark\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"DataCleaning\").getOrCreate()\n",
    "\n",
    "# Ruta del archivo cargado\n",
    "file_path = \"C:\\\\Users\\\\Usuario\\\\Documents\\\\WILLIAM\\\\DOCUMENTOS\\\\PERSONALES\\\\MONOKERA\\\\MOCK_DATA.csv\"\n",
    "\n",
    "# Leer el archivo CSV y asegurarse de que tiene encabezado y tipo de datos inferido\n",
    "data_3 = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Verificar los nombres de las columnas\n",
    "print(\"Column names: \", data_3.columns)\n",
    "\n",
    "# Limpiar la columna 'id' para extraer solo los números (si están en el formato esperado)\n",
    "# Usamos expresiones regulares para extraer solo la parte numérica\n",
    "data_3 = data_3.withColumn('id', regexp_extract(col('id'), r'\\d+', 0))\n",
    "\n",
    "# Filtrar filas donde 'id' no es nulo y contiene solo números\n",
    "data_3_cleaned = data_3.filter(col('id').rlike('^\\d+$'))\n",
    "\n",
    "# Verificar si las columnas de fecha existen y convertirlas al formato correcto\n",
    "date_columns = [\"claim_date\", \"payment_date\"]\n",
    "for col_name in date_columns:\n",
    "    if col_name in data_3_cleaned.columns:\n",
    "        data_3_cleaned = data_3_cleaned.withColumn(col_name, to_date(col(col_name), \"M/d/yyyy\"))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada\")\n",
    "\n",
    "# Manejo de valores nulos\n",
    "# Rellenar valores nulos con valores predeterminados\n",
    "data_cleaned = data_3_cleaned.fillna({\n",
    "    \"insured_age\": 0,\n",
    "    \"insured_gender\": \"Unknown\",\n",
    "    \"claim_status\": \"Unknown\",\n",
    "    \"payment_status\": \"Unknown\",\n",
    "    \"payment_method\": \"Unknown\"\n",
    "})\n",
    "\n",
    "# Convertir columnas numéricas relevantes a tipos adecuados\n",
    "numeric_columns = [\"claim_amount\", \"payment_amount\"]\n",
    "for col_name in numeric_columns:\n",
    "    if col_name in data_cleaned.columns:\n",
    "        data_cleaned = data_cleaned.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada para conversión a tipo numérico\")\n",
    "\n",
    "# Mostrar las primeras filas después de limpieza\n",
    "data_cleaned.show(5)\n",
    "\n",
    "# **CONEXIÓN A SQL SERVER Y ESCRITURA DE DATOS**\n",
    "\n",
    "# Configuración de la conexión a SQL Server\n",
    "jdbc_url = \"jdbc:sqlserver://LAPTOP-O07NV287:1433;databaseName=monokera\"\n",
    "properties = {\n",
    "    \"user\": \"williamxlr\",\n",
    "    \"password\": \"Al3xW$1978\",\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "# **1. Insertar en la tabla 'Policy'**\n",
    "policy_data = data_cleaned.select(\"policy_number\", \"policy_start_date\", \"policy_end_date\", \"policy_type\", \"insurance_company\")\n",
    "policy_data.write.jdbc(url=jdbc_url, table=\"Policy\", mode=\"append\", properties=properties)\n",
    "\n",
    "# **2. Insertar en la tabla 'Insured'**\n",
    "insured_data = data_cleaned.select(\"insured_id\", \"policy_number\", \"insured_name\", \"insured_gender\", \"insured_age\",\n",
    "                                   \"insured_address\", \"insured_city\", \"insured_state\", \"insured_postal_code\", \"insured_country\")\n",
    "insured_data.write.jdbc(url=jdbc_url, table=\"Insured\", mode=\"append\", properties=properties)\n",
    "\n",
    "# **3. Insertar en la tabla 'Premium'**\n",
    "premium_data = data_cleaned.select(\"policy_number\", \"premium_amount\", \"deductible_amount\", \"coverage_limit\")\n",
    "premium_data.write.jdbc(url=jdbc_url, table=\"Premium\", mode=\"append\", properties=properties)\n",
    "\n",
    "# **4. Insertar en la tabla 'Payments'**\n",
    "payments_data = data_cleaned.select(\"policy_number\", \"payment_status\", \"payment_date\", \"payment_amount\", \"payment_method\")\n",
    "payments_data.write.jdbc(url=jdbc_url, table=\"Payments\", mode=\"append\", properties=properties)\n",
    "\n",
    "# **5. Insertar en la tabla 'Claims'**\n",
    "claims_data = data_cleaned.select(\"policy_number\", \"claim_status\", \"claim_date\", \"claim_amount\", \"claim_description\")\n",
    "claims_data.write.jdbc(url=jdbc_url, table=\"Claims\", mode=\"append\", properties=properties)\n",
    "\n",
    "# **6. Insertar en la tabla 'Agents'**\n",
    "agents_data = data_cleaned.select(\"policy_number\", \"agent_name\", \"agent_email\", \"agent_phone\")\n",
    "agents_data.write.jdbc(url=jdbc_url, table=\"Agents\", mode=\"append\", properties=properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:  ['id', 'first_name', 'last_name', 'email', 'gender', 'policy_number', 'insured_name', 'insured_gender', 'insured_age', 'insured_address', 'insured_city', 'insured_state', 'insured_postal_code', 'insured_country', 'policy_start_date', 'policy_end_date', 'premium_amount', 'deductible_amount', 'coverage_limit', 'policy_type', 'insurance_company', 'agent_name', 'agent_email', 'agent_phone', 'claim_status', 'claim_date', 'claim_amount', 'claim_description', 'payment_status', 'payment_date', 'payment_amount', 'payment_method']\n",
      "+---+----------+---------+--------------------+------+-------------+----------------+--------------+-----------+--------------------+------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "| id|first_name|last_name|               email|gender|policy_number|    insured_name|insured_gender|insured_age|     insured_address|insured_city|insured_state|insured_postal_code|insured_country|policy_start_date|policy_end_date|premium_amount|deductible_amount|coverage_limit|policy_type|insurance_company|      agent_name|         agent_email|      agent_phone|claim_status|claim_date|claim_amount|   claim_description|payment_status|payment_date|payment_amount|payment_method|\n",
      "+---+----------+---------+--------------------+------+-------------+----------------+--------------+-----------+--------------------+------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "|293|     Lorne|    Denne| ldenne84@smh.com.au|  Male|          293|     Lorne Denne|          Male|         33|28393 Goodland Cr...|   Davenport|         Iowa|              52804|  United States|       2023-06-19|     01/11/2024|       2251.36|           638.48|     351587.59|       Home|         Linkbuzz|     Lorne Denne|ldenne84@shop-pro.jp|+1 (563) 596-5970|    Approved|2022-10-26|     3695.83|Etiam vel augue. ...|       Pending|  2022-06-01|        447.76|         Check|\n",
      "|915|    Ralina|    Gabel|rgabelpe@vkontakt...|  Male|          915|    Ralina Gabel|        Female|          0|9844 Claremont Av...| Springfield|     Missouri|              65898|  United States|       2023-02-08|     08/19/2023|       5412.31|          3491.01|      551417.7|       Life|           Zoozzy|    Ralina Gabel|    rgabelpe@mlb.com|+1 (417) 562-6585|     Pending|2022-01-24|     8411.61|Aenean lectus. Pe...|          Paid|  2022-10-06|        748.59| Bank Transfer|\n",
      "|917|    Jessey|  Panting|jpantingpg@state....|Female|          917|  Jessey Panting|   Genderqueer|         58|    74170 Nelson Way|   Nashville|    Tennessee|              37220|  United States|       2023-08-22|     10/14/2023|       4016.36|           816.55|     329815.95|       Auto|            Quire|  Jessey Panting|jpantingpg@blogs.com|+1 (615) 257-3106|    Approved|2022-09-29|     4838.33|Sed ante. Vivamus...|       Pending|  2022-06-03|         698.4|         Check|\n",
      "|533|   Fremont| Baversor| fbaversores@com.com|Female|          533|Fremont Baversor|          Male|         19|       167 Lien Road|        Lima|         Ohio|              45807|  United States|       2023-10-23|     03/13/2023|       1484.06|          2885.32|      377748.2|     Health|             Yotz|Fremont Baversor|fbaversores@studi...|+1 (419) 509-1666|    Approved|2022-12-17|     7629.76|Quisque id justo ...|          Paid|  2022-09-12|        515.01| Bank Transfer|\n",
      "|677|    Cathie|Kingsford|ckingsfordis@usgs...|  Male|          677|Cathie Kingsford|        Female|         40|596 Loeprich Junc...|    Sarasota|      Florida|              34233|  United States|       2024-01-24|     12/03/2023|        5194.9|          4326.98|     994158.85|       Life|            Jaloo|Cathie Kingsford|ckingsfordis@yell...|+1 (941) 606-1916|    Approved|2022-06-17|     8904.59|Duis consequat du...|       Pending|  2022-05-16|         972.9|   Credit Card|\n",
      "+---+----------+---------+--------------------+------+-------------+----------------+--------------+-----------+--------------------+------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1039.jdbc.\n: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\r\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:766)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 71\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Verificar duplicados antes de insertar en 'Policy'\u001b[39;00m\n\u001b[0;32m     70\u001b[0m policy_data \u001b[38;5;241m=\u001b[39m policy_data\u001b[38;5;241m.\u001b[39mdropDuplicates([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_number\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 71\u001b[0m \u001b[43mpolicy_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjdbc_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPolicy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproperties\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# **2. Insertar en la tabla 'Insured'**\u001b[39;00m\n\u001b[0;32m     74\u001b[0m insured_data \u001b[38;5;241m=\u001b[39m data_cleaned\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_number\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_gender\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_age\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     75\u001b[0m                                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_address\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_city\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_postal_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_country\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\readwriter.py:1984\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[1;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[0;32m   1982\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[0;32m   1983\u001b[0m     jprop\u001b[38;5;241m.\u001b[39msetProperty(k, properties[k])\n\u001b[1;32m-> 1984\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1039.jdbc.\n: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\r\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:766)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, regexp_extract\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Inicializa la sesión de Spark\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"DataCleaning\").getOrCreate()\n",
    "\n",
    "# Ruta del archivo cargado\n",
    "file_path = \"C:\\\\Users\\\\Usuario\\\\Documents\\\\WILLIAM\\\\DOCUMENTOS\\\\PERSONALES\\\\MONOKERA\\\\MOCK_DATA.csv\"\n",
    "\n",
    "# Leer el archivo CSV y asegurarse de que tiene encabezado y tipo de datos inferido\n",
    "data_3 = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Verificar los nombres de las columnas\n",
    "print(\"Column names: \", data_3.columns)\n",
    "\n",
    "# Limpiar la columna 'id' para extraer solo los números (si están en el formato esperado)\n",
    "# Usamos expresiones regulares para extraer solo la parte numérica\n",
    "data_3 = data_3.withColumn('id', regexp_extract(col('id'), r'\\d+', 0))\n",
    "\n",
    "# Filtrar filas donde 'id' no es nulo y contiene solo números\n",
    "data_3_cleaned = data_3.filter(col('id').rlike('^\\d+$'))\n",
    "\n",
    "# Verificar si las columnas de fecha existen y convertirlas al formato correcto\n",
    "date_columns = [\"claim_date\", \"payment_date\"]\n",
    "for col_name in date_columns:\n",
    "    if col_name in data_3_cleaned.columns:\n",
    "        data_3_cleaned = data_3_cleaned.withColumn(col_name, to_date(col(col_name), \"M/d/yyyy\"))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada\")\n",
    "\n",
    "# Manejo de valores nulos\n",
    "# Rellenar valores nulos con valores predeterminados\n",
    "data_cleaned = data_3_cleaned.fillna({\n",
    "    \"insured_age\": 0,\n",
    "    \"insured_gender\": \"Unknown\",\n",
    "    \"claim_status\": \"Unknown\",\n",
    "    \"payment_status\": \"Unknown\",\n",
    "    \"payment_method\": \"Unknown\"\n",
    "})\n",
    "\n",
    "# Convertir columnas numéricas relevantes a tipos adecuados\n",
    "numeric_columns = [\"claim_amount\", \"payment_amount\"]\n",
    "for col_name in numeric_columns:\n",
    "    if col_name in data_cleaned.columns:\n",
    "        data_cleaned = data_cleaned.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada para conversión a tipo numérico\")\n",
    "\n",
    "# Eliminar duplicados antes de insertar (especialmente para claves primarias)\n",
    "data_cleaned = data_cleaned.dropDuplicates()\n",
    "\n",
    "# Mostrar las primeras filas después de limpieza\n",
    "data_cleaned.show(5)\n",
    "\n",
    "# **CONEXIÓN A SQL SERVER Y ESCRITURA DE DATOS**\n",
    "\n",
    "# Configuración de la conexión a SQL Server\n",
    "jdbc_url = \"jdbc:sqlserver://LAPTOP-O07NV287:1433;databaseName=monokera\"\n",
    "properties = {\n",
    "    \"user\": \"williamxlr\",\n",
    "    \"password\": \"Al3xW$1978\",\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "# **1. Insertar en la tabla 'Policy'**\n",
    "policy_data = data_cleaned.select(\"policy_number\", \"policy_start_date\", \"policy_end_date\", \"policy_type\", \"insurance_company\")\n",
    "# Verificar duplicados antes de insertar en 'Policy'\n",
    "policy_data = policy_data.dropDuplicates([\"policy_number\"])\n",
    "policy_data.write.jdbc(url=jdbc_url, table=\"Policy\", mode=\"append\", properties=properties)\n",
    "\n",
    "# **2. Insertar en la tabla 'Insured'**\n",
    "insured_data = data_cleaned.select(\"insured_id\", \"policy_number\", \"insured_name\", \"insured_gender\", \"insured_age\",\n",
    "                                   \"insured_address\", \"insured_city\", \"insured_state\", \"insured_postal_code\", \"insured_country\")\n",
    "# Verificar duplicados antes de insertar en 'Insured'\n",
    "insured_data = insured_data.dropDuplicates([\"insured_id\"])\n",
    "insured_data.write.jdbc(url=jdbc_url, table=\"Insured\", mode=\"overwrite\", properties=properties)\n",
    "\n",
    "# **3. Insertar en la tabla 'Premium'**\n",
    "premium_data = data_cleaned.select(\"policy_number\", \"premium_amount\", \"deductible_amount\", \"coverage_limit\")\n",
    "# Verificar duplicados antes de insertar en 'Premium'\n",
    "premium_data = premium_data.dropDuplicates([\"policy_number\"])\n",
    "premium_data.write.jdbc(url=jdbc_url, table=\"Premium\", mode=\"overwrite\", properties=properties)\n",
    "\n",
    "# **4. Insertar en la tabla 'Payments'**\n",
    "payments_data = data_cleaned.select(\"policy_number\", \"payment_status\", \"payment_date\", \"payment_amount\", \"payment_method\")\n",
    "# Verificar duplicados antes de insertar en 'Payments'\n",
    "payments_data = payments_data.dropDuplicates([\"policy_number\", \"payment_date\"])  # Asumir que una combinación de estos es única\n",
    "payments_data.write.jdbc(url=jdbc_url, table=\"Payments\", mode=\"overwrite\", properties=properties)\n",
    "\n",
    "# **5. Insertar en la tabla 'Claims'**\n",
    "claims_data = data_cleaned.select(\"policy_number\", \"claim_status\", \"claim_date\", \"claim_amount\", \"claim_description\")\n",
    "# Verificar duplicados antes de insertar en 'Claims'\n",
    "claims_data = claims_data.dropDuplicates([\"policy_number\", \"claim_date\"])\n",
    "claims_data.write.jdbc(url=jdbc_url, table=\"Claims\", mode=\"overwrite\", properties=properties)\n",
    "\n",
    "# **6. Insertar en la tabla 'Agents'**\n",
    "agents_data = data_cleaned.select(\"policy_number\", \"agent_name\", \"agent_email\", \"agent_phone\")\n",
    "# Verificar duplicados antes de insertar en 'Agents'\n",
    "agents_data = agents_data.dropDuplicates([\"agent_email\"])  # Suponiendo que el correo electrónico es único\n",
    "agents_data.write.jdbc(url=jdbc_url, table=\"Agents\", mode=\"overwrite\", properties=properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:  ['id', 'first_name', 'last_name', 'email', 'gender', 'policy_number', 'insured_name', 'insured_gender', 'insured_age', 'insured_address', 'insured_city', 'insured_state', 'insured_postal_code', 'insured_country', 'policy_start_date', 'policy_end_date', 'premium_amount', 'deductible_amount', 'coverage_limit', 'policy_type', 'insurance_company', 'agent_name', 'agent_email', 'agent_phone', 'claim_status', 'claim_date', 'claim_amount', 'claim_description', 'payment_status', 'payment_date', 'payment_amount', 'payment_method']\n",
      "+---+----------+---------+--------------------+------+-------------+----------------+--------------+-----------+--------------------+------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "| id|first_name|last_name|               email|gender|policy_number|    insured_name|insured_gender|insured_age|     insured_address|insured_city|insured_state|insured_postal_code|insured_country|policy_start_date|policy_end_date|premium_amount|deductible_amount|coverage_limit|policy_type|insurance_company|      agent_name|         agent_email|      agent_phone|claim_status|claim_date|claim_amount|   claim_description|payment_status|payment_date|payment_amount|payment_method|\n",
      "+---+----------+---------+--------------------+------+-------------+----------------+--------------+-----------+--------------------+------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "|293|     Lorne|    Denne| ldenne84@smh.com.au|  Male|          293|     Lorne Denne|          Male|         33|28393 Goodland Cr...|   Davenport|         Iowa|              52804|  United States|       2023-06-19|     01/11/2024|       2251.36|           638.48|     351587.59|       Home|         Linkbuzz|     Lorne Denne|ldenne84@shop-pro.jp|+1 (563) 596-5970|    Approved|2022-10-26|     3695.83|Etiam vel augue. ...|       Pending|  2022-06-01|        447.76|         Check|\n",
      "|915|    Ralina|    Gabel|rgabelpe@vkontakt...|  Male|          915|    Ralina Gabel|        Female|          0|9844 Claremont Av...| Springfield|     Missouri|              65898|  United States|       2023-02-08|     08/19/2023|       5412.31|          3491.01|      551417.7|       Life|           Zoozzy|    Ralina Gabel|    rgabelpe@mlb.com|+1 (417) 562-6585|     Pending|2022-01-24|     8411.61|Aenean lectus. Pe...|          Paid|  2022-10-06|        748.59| Bank Transfer|\n",
      "|917|    Jessey|  Panting|jpantingpg@state....|Female|          917|  Jessey Panting|   Genderqueer|         58|    74170 Nelson Way|   Nashville|    Tennessee|              37220|  United States|       2023-08-22|     10/14/2023|       4016.36|           816.55|     329815.95|       Auto|            Quire|  Jessey Panting|jpantingpg@blogs.com|+1 (615) 257-3106|    Approved|2022-09-29|     4838.33|Sed ante. Vivamus...|       Pending|  2022-06-03|         698.4|         Check|\n",
      "|533|   Fremont| Baversor| fbaversores@com.com|Female|          533|Fremont Baversor|          Male|         19|       167 Lien Road|        Lima|         Ohio|              45807|  United States|       2023-10-23|     03/13/2023|       1484.06|          2885.32|      377748.2|     Health|             Yotz|Fremont Baversor|fbaversores@studi...|+1 (419) 509-1666|    Approved|2022-12-17|     7629.76|Quisque id justo ...|          Paid|  2022-09-12|        515.01| Bank Transfer|\n",
      "|677|    Cathie|Kingsford|ckingsfordis@usgs...|  Male|          677|Cathie Kingsford|        Female|         40|596 Loeprich Junc...|    Sarasota|      Florida|              34233|  United States|       2024-01-24|     12/03/2023|        5194.9|          4326.98|     994158.85|       Life|            Jaloo|Cathie Kingsford|ckingsfordis@yell...|+1 (941) 606-1916|    Approved|2022-06-17|     8904.59|Duis consequat du...|       Pending|  2022-05-16|         972.9|   Credit Card|\n",
      "+---+----------+---------+--------------------+------+-------------+----------------+--------------+-----------+--------------------+------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1289.jdbc.\n: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\r\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:766)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Verificar duplicados antes de insertar en 'Policy'\u001b[39;00m\n\u001b[0;32m     75\u001b[0m policy_data \u001b[38;5;241m=\u001b[39m policy_data\u001b[38;5;241m.\u001b[39mdropDuplicates([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_number\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 76\u001b[0m \u001b[43mpolicy_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjdbc_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPolicy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproperties\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# **2. Insertar en la tabla 'Insured'**\u001b[39;00m\n\u001b[0;32m     79\u001b[0m insured_data \u001b[38;5;241m=\u001b[39m data_cleaned\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_number\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_gender\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_age\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     80\u001b[0m                                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_address\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_city\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_postal_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_country\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\readwriter.py:1984\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[1;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[0;32m   1982\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[0;32m   1983\u001b[0m     jprop\u001b[38;5;241m.\u001b[39msetProperty(k, properties[k])\n\u001b[1;32m-> 1984\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1289.jdbc.\n: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\r\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:766)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, regexp_extract\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Inicia Spark con el archivo .jar del JDBC para conectarse a SQL Server\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"DataCleaning\") \\\n",
    "    .config(\"spark.jars\", \"C:/spark/jars/mssql-jdbc-10.2.0.jre8.jar\")  \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Ruta del archivo cargado\n",
    "file_path = \"C:\\\\Users\\\\Usuario\\\\Documents\\\\WILLIAM\\\\DOCUMENTOS\\\\PERSONALES\\\\MONOKERA\\\\MOCK_DATA.csv\"\n",
    "\n",
    "# Leer el archivo CSV y asegurarse de que tiene encabezado y tipo de datos inferido\n",
    "data_3 = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Verificar los nombres de las columnas\n",
    "print(\"Column names: \", data_3.columns)\n",
    "\n",
    "# Limpiar la columna 'id' para extraer solo los números (si están en el formato esperado)\n",
    "# Usamos expresiones regulares para extraer solo la parte numérica\n",
    "data_3 = data_3.withColumn('id', regexp_extract(col('id'), r'\\d+', 0))\n",
    "\n",
    "# Filtrar filas donde 'id' no es nulo y contiene solo números\n",
    "data_3_cleaned = data_3.filter(col('id').rlike('^\\d+$'))\n",
    "\n",
    "# Verificar si las columnas de fecha existen y convertirlas al formato correcto\n",
    "date_columns = [\"claim_date\", \"payment_date\"]\n",
    "for col_name in date_columns:\n",
    "    if col_name in data_3_cleaned.columns:\n",
    "        data_3_cleaned = data_3_cleaned.withColumn(col_name, to_date(col(col_name), \"M/d/yyyy\"))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada\")\n",
    "\n",
    "# Manejo de valores nulos\n",
    "# Rellenar valores nulos con valores predeterminados\n",
    "data_cleaned = data_3_cleaned.fillna({\n",
    "    \"insured_age\": 0,\n",
    "    \"insured_gender\": \"Unknown\",\n",
    "    \"claim_status\": \"Unknown\",\n",
    "    \"payment_status\": \"Unknown\",\n",
    "    \"payment_method\": \"Unknown\"\n",
    "})\n",
    "\n",
    "# Convertir columnas numéricas relevantes a tipos adecuados\n",
    "numeric_columns = [\"claim_amount\", \"payment_amount\"]\n",
    "for col_name in numeric_columns:\n",
    "    if col_name in data_cleaned.columns:\n",
    "        data_cleaned = data_cleaned.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada para conversión a tipo numérico\")\n",
    "\n",
    "# Eliminar duplicados antes de insertar (especialmente para claves primarias)\n",
    "data_cleaned = data_cleaned.dropDuplicates()\n",
    "\n",
    "# Mostrar las primeras filas después de limpieza\n",
    "data_cleaned.show(5)\n",
    "\n",
    "# **CONEXIÓN A SQL SERVER Y ESCRITURA DE DATOS**\n",
    "\n",
    "# Configuración de la conexión a SQL Server\n",
    "jdbc_url = \"jdbc:sqlserver://LAPTOP-O07NV287:1433;databaseName=monokera\"\n",
    "properties = {\n",
    "    \"user\": \"williamxlr\",\n",
    "    \"password\": \"Al3xW$1978\",\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "# **1. Insertar en la tabla 'Policy'**\n",
    "policy_data = data_cleaned.select(\"policy_number\", \"policy_start_date\", \"policy_end_date\", \"policy_type\", \"insurance_company\")\n",
    "# Verificar duplicados antes de insertar en 'Policy'\n",
    "policy_data = policy_data.dropDuplicates([\"policy_number\"])\n",
    "policy_data.write.jdbc(url=jdbc_url, table=\"Policy\", mode=\"overwrite\", properties=properties)\n",
    "\n",
    "# **2. Insertar en la tabla 'Insured'**\n",
    "insured_data = data_cleaned.select(\"insured_id\", \"policy_number\", \"insured_name\", \"insured_gender\", \"insured_age\",\n",
    "                                   \"insured_address\", \"insured_city\", \"insured_state\", \"insured_postal_code\", \"insured_country\")\n",
    "# Verificar duplicados antes de insertar en 'Insured'\n",
    "insured_data = insured_data.dropDuplicates([\"insured_id\"])\n",
    "insured_data.write.jdbc(url=jdbc_url, table=\"Insured\", mode=\"overwrite\", properties=properties)\n",
    "\n",
    "# **3. Insertar en la tabla 'Premium'**\n",
    "premium_data = data_cleaned.select(\"policy_number\", \"premium_amount\", \"deductible_amount\", \"coverage_limit\")\n",
    "# Verificar duplicados antes de insertar en 'Premium'\n",
    "premium_data = premium_data.dropDuplicates([\"policy_number\"])\n",
    "premium_data.write.jdbc(url=jdbc_url, table=\"Premium\", mode=\"overwrite\", properties=properties)\n",
    "\n",
    "# **4. Insertar en la tabla 'Payments'**\n",
    "payments_data = data_cleaned.select(\"policy_number\", \"payment_status\", \"payment_date\", \"payment_amount\", \"payment_method\")\n",
    "# Verificar duplicados antes de insertar en 'Payments'\n",
    "payments_data = payments_data.dropDuplicates([\"policy_number\", \"payment_date\"])  # Asumir que una combinación de estos es única\n",
    "payments_data.write.jdbc(url=jdbc_url, table=\"Payments\", mode=\"overwrite\", properties=properties)\n",
    "\n",
    "# **5. Insertar en la tabla 'Claims'**\n",
    "claims_data = data_cleaned.select(\"policy_number\", \"claim_status\", \"claim_date\", \"claim_amount\", \"claim_description\")\n",
    "# Verificar duplicados antes de insertar en 'Claims'\n",
    "claims_data = claims_data.dropDuplicates([\"policy_number\", \"claim_date\"])\n",
    "claims_data.write.jdbc(url=jdbc_url, table=\"Claims\", mode=\"overwrite\", properties=properties)\n",
    "\n",
    "# **6. Insertar en la tabla 'Agents'**\n",
    "agents_data = data_cleaned.select(\"policy_number\", \"agent_name\", \"agent_email\", \"agent_phone\")\n",
    "# Verificar duplicados antes de insertar en 'Agents'\n",
    "agents_data = agents_data.dropDuplicates([\"agent_email\"])  # Suponiendo que el correo electrónico es único\n",
    "agents_data.write.jdbc(url=jdbc_url, table=\"Agents\", mode=\"overwrite\", properties=properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:  ['id', 'first_name', 'last_name', 'email', 'gender', 'policy_number', 'insured_name', 'insured_gender', 'insured_age', 'insured_address', 'insured_city', 'insured_state', 'insured_postal_code', 'insured_country', 'policy_start_date', 'policy_end_date', 'premium_amount', 'deductible_amount', 'coverage_limit', 'policy_type', 'insurance_company', 'agent_name', 'agent_email', 'agent_phone', 'claim_status', 'claim_date', 'claim_amount', 'claim_description', 'payment_status', 'payment_date', 'payment_amount', 'payment_method']\n",
      "+---+----------+---------+--------------------+------+-------------+----------------+--------------+-----------+--------------------+------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "| id|first_name|last_name|               email|gender|policy_number|    insured_name|insured_gender|insured_age|     insured_address|insured_city|insured_state|insured_postal_code|insured_country|policy_start_date|policy_end_date|premium_amount|deductible_amount|coverage_limit|policy_type|insurance_company|      agent_name|         agent_email|      agent_phone|claim_status|claim_date|claim_amount|   claim_description|payment_status|payment_date|payment_amount|payment_method|\n",
      "+---+----------+---------+--------------------+------+-------------+----------------+--------------+-----------+--------------------+------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "|293|     Lorne|    Denne| ldenne84@smh.com.au|  Male|          293|     Lorne Denne|          Male|         33|28393 Goodland Cr...|   Davenport|         Iowa|              52804|  United States|       2023-06-19|     01/11/2024|       2251.36|           638.48|     351587.59|       Home|         Linkbuzz|     Lorne Denne|ldenne84@shop-pro.jp|+1 (563) 596-5970|    Approved|2022-10-26|     3695.83|Etiam vel augue. ...|       Pending|  2022-06-01|        447.76|         Check|\n",
      "|915|    Ralina|    Gabel|rgabelpe@vkontakt...|  Male|          915|    Ralina Gabel|        Female|          0|9844 Claremont Av...| Springfield|     Missouri|              65898|  United States|       2023-02-08|     08/19/2023|       5412.31|          3491.01|      551417.7|       Life|           Zoozzy|    Ralina Gabel|    rgabelpe@mlb.com|+1 (417) 562-6585|     Pending|2022-01-24|     8411.61|Aenean lectus. Pe...|          Paid|  2022-10-06|        748.59| Bank Transfer|\n",
      "|917|    Jessey|  Panting|jpantingpg@state....|Female|          917|  Jessey Panting|   Genderqueer|         58|    74170 Nelson Way|   Nashville|    Tennessee|              37220|  United States|       2023-08-22|     10/14/2023|       4016.36|           816.55|     329815.95|       Auto|            Quire|  Jessey Panting|jpantingpg@blogs.com|+1 (615) 257-3106|    Approved|2022-09-29|     4838.33|Sed ante. Vivamus...|       Pending|  2022-06-03|         698.4|         Check|\n",
      "|533|   Fremont| Baversor| fbaversores@com.com|Female|          533|Fremont Baversor|          Male|         19|       167 Lien Road|        Lima|         Ohio|              45807|  United States|       2023-10-23|     03/13/2023|       1484.06|          2885.32|      377748.2|     Health|             Yotz|Fremont Baversor|fbaversores@studi...|+1 (419) 509-1666|    Approved|2022-12-17|     7629.76|Quisque id justo ...|          Paid|  2022-09-12|        515.01| Bank Transfer|\n",
      "|677|    Cathie|Kingsford|ckingsfordis@usgs...|  Male|          677|Cathie Kingsford|        Female|         40|596 Loeprich Junc...|    Sarasota|      Florida|              34233|  United States|       2024-01-24|     12/03/2023|        5194.9|          4326.98|     994158.85|       Life|            Jaloo|Cathie Kingsford|ckingsfordis@yell...|+1 (941) 606-1916|    Approved|2022-06-17|     8904.59|Duis consequat du...|       Pending|  2022-05-16|         972.9|   Credit Card|\n",
      "+---+----------+---------+--------------------+------+-------------+----------------+--------------+-----------+--------------------+------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `insured_id` cannot be resolved. Did you mean one of the following? [`insured_age`, `insured_city`, `insured_name`, `insured_gender`, `insured_state`].;\n'Project ['insured_id, policy_number#13060, insured_name#13061, insured_gender#13251, insured_age#13252, insured_address#13064, insured_city#13065, insured_state#13066, insured_postal_code#13067, insured_country#13068]\n+- Deduplicate [agent_email#13077, claim_description#13082, insurance_company#13075, first_name#13056, insured_age#13252, email#13058, coverage_limit#13073, insured_name#13061, agent_name#13076, claim_date#13153, insured_city#13065, payment_date#13186, claim_status#13253, insured_country#13068, insured_address#13064, insured_state#13066, payment_method#13255, payment_status#13254, id#13119, policy_type#13074, last_name#13057, policy_end_date#13070, insured_gender#13251, policy_start_date#13069, ... 8 more fields]\n   +- Project [id#13119, first_name#13056, last_name#13057, email#13058, gender#13059, policy_number#13060, insured_name#13061, insured_gender#13251, insured_age#13252, insured_address#13064, insured_city#13065, insured_state#13066, insured_postal_code#13067, insured_country#13068, policy_start_date#13069, policy_end_date#13070, premium_amount#13071, deductible_amount#13072, coverage_limit#13073, policy_type#13074, insurance_company#13075, agent_name#13076, agent_email#13077, agent_phone#13078, ... 8 more fields]\n      +- Project [id#13119, first_name#13056, last_name#13057, email#13058, gender#13059, policy_number#13060, insured_name#13061, insured_gender#13251, insured_age#13252, insured_address#13064, insured_city#13065, insured_state#13066, insured_postal_code#13067, insured_country#13068, policy_start_date#13069, policy_end_date#13070, premium_amount#13071, deductible_amount#13072, coverage_limit#13073, policy_type#13074, insurance_company#13075, agent_name#13076, agent_email#13077, agent_phone#13078, ... 8 more fields]\n         +- Project [id#13119, first_name#13056, last_name#13057, email#13058, gender#13059, policy_number#13060, insured_name#13061, coalesce(insured_gender#13062, cast(Unknown as string)) AS insured_gender#13251, coalesce(insured_age#13063, cast(0 as string)) AS insured_age#13252, insured_address#13064, insured_city#13065, insured_state#13066, insured_postal_code#13067, insured_country#13068, policy_start_date#13069, policy_end_date#13070, premium_amount#13071, deductible_amount#13072, coverage_limit#13073, policy_type#13074, insurance_company#13075, agent_name#13076, agent_email#13077, agent_phone#13078, ... 8 more fields]\n            +- Project [id#13119, first_name#13056, last_name#13057, email#13058, gender#13059, policy_number#13060, insured_name#13061, insured_gender#13062, insured_age#13063, insured_address#13064, insured_city#13065, insured_state#13066, insured_postal_code#13067, insured_country#13068, policy_start_date#13069, policy_end_date#13070, premium_amount#13071, deductible_amount#13072, coverage_limit#13073, policy_type#13074, insurance_company#13075, agent_name#13076, agent_email#13077, agent_phone#13078, ... 8 more fields]\n               +- Project [id#13119, first_name#13056, last_name#13057, email#13058, gender#13059, policy_number#13060, insured_name#13061, insured_gender#13062, insured_age#13063, insured_address#13064, insured_city#13065, insured_state#13066, insured_postal_code#13067, insured_country#13068, policy_start_date#13069, policy_end_date#13070, premium_amount#13071, deductible_amount#13072, coverage_limit#13073, policy_type#13074, insurance_company#13075, agent_name#13076, agent_email#13077, agent_phone#13078, ... 8 more fields]\n                  +- Filter RLIKE(id#13119, ^\\d+$)\n                     +- Project [regexp_extract(id#13055, \\d+, 0) AS id#13119, first_name#13056, last_name#13057, email#13058, gender#13059, policy_number#13060, insured_name#13061, insured_gender#13062, insured_age#13063, insured_address#13064, insured_city#13065, insured_state#13066, insured_postal_code#13067, insured_country#13068, policy_start_date#13069, policy_end_date#13070, premium_amount#13071, deductible_amount#13072, coverage_limit#13073, policy_type#13074, insurance_company#13075, agent_name#13076, agent_email#13077, agent_phone#13078, ... 8 more fields]\n                        +- Relation [id#13055,first_name#13056,last_name#13057,email#13058,gender#13059,policy_number#13060,insured_name#13061,insured_gender#13062,insured_age#13063,insured_address#13064,insured_city#13065,insured_state#13066,insured_postal_code#13067,insured_country#13068,policy_start_date#13069,policy_end_date#13070,premium_amount#13071,deductible_amount#13072,coverage_limit#13073,policy_type#13074,insurance_company#13075,agent_name#13076,agent_email#13077,agent_phone#13078,... 8 more fields] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 80\u001b[0m\n\u001b[0;32m     74\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124m        INSERT INTO Policy (policy_number, policy_start_date, policy_end_date, policy_type, insurance_company)\u001b[39m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124m        VALUES (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m, (row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy_number\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy_start_date\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy_end_date\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy_type\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minsurance_company\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# **2. Insertar en la tabla 'Insured'**\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m insured_data \u001b[38;5;241m=\u001b[39m \u001b[43mdata_cleaned\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minsured_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpolicy_number\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minsured_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minsured_gender\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minsured_age\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minsured_address\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minsured_city\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minsured_state\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minsured_postal_code\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minsured_country\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m insured_data \u001b[38;5;241m=\u001b[39m insured_data\u001b[38;5;241m.\u001b[39mdropDuplicates([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m insured_data\u001b[38;5;241m.\u001b[39mcollect():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\dataframe.py:3229\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   3184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   3185\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   3186\u001b[0m \n\u001b[0;32m   3187\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3227\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[0;32m   3228\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3229\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `insured_id` cannot be resolved. Did you mean one of the following? [`insured_age`, `insured_city`, `insured_name`, `insured_gender`, `insured_state`].;\n'Project ['insured_id, policy_number#13060, insured_name#13061, insured_gender#13251, insured_age#13252, insured_address#13064, insured_city#13065, insured_state#13066, insured_postal_code#13067, insured_country#13068]\n+- Deduplicate [agent_email#13077, claim_description#13082, insurance_company#13075, first_name#13056, insured_age#13252, email#13058, coverage_limit#13073, insured_name#13061, agent_name#13076, claim_date#13153, insured_city#13065, payment_date#13186, claim_status#13253, insured_country#13068, insured_address#13064, insured_state#13066, payment_method#13255, payment_status#13254, id#13119, policy_type#13074, last_name#13057, policy_end_date#13070, insured_gender#13251, policy_start_date#13069, ... 8 more fields]\n   +- Project [id#13119, first_name#13056, last_name#13057, email#13058, gender#13059, policy_number#13060, insured_name#13061, insured_gender#13251, insured_age#13252, insured_address#13064, insured_city#13065, insured_state#13066, insured_postal_code#13067, insured_country#13068, policy_start_date#13069, policy_end_date#13070, premium_amount#13071, deductible_amount#13072, coverage_limit#13073, policy_type#13074, insurance_company#13075, agent_name#13076, agent_email#13077, agent_phone#13078, ... 8 more fields]\n      +- Project [id#13119, first_name#13056, last_name#13057, email#13058, gender#13059, policy_number#13060, insured_name#13061, insured_gender#13251, insured_age#13252, insured_address#13064, insured_city#13065, insured_state#13066, insured_postal_code#13067, insured_country#13068, policy_start_date#13069, policy_end_date#13070, premium_amount#13071, deductible_amount#13072, coverage_limit#13073, policy_type#13074, insurance_company#13075, agent_name#13076, agent_email#13077, agent_phone#13078, ... 8 more fields]\n         +- Project [id#13119, first_name#13056, last_name#13057, email#13058, gender#13059, policy_number#13060, insured_name#13061, coalesce(insured_gender#13062, cast(Unknown as string)) AS insured_gender#13251, coalesce(insured_age#13063, cast(0 as string)) AS insured_age#13252, insured_address#13064, insured_city#13065, insured_state#13066, insured_postal_code#13067, insured_country#13068, policy_start_date#13069, policy_end_date#13070, premium_amount#13071, deductible_amount#13072, coverage_limit#13073, policy_type#13074, insurance_company#13075, agent_name#13076, agent_email#13077, agent_phone#13078, ... 8 more fields]\n            +- Project [id#13119, first_name#13056, last_name#13057, email#13058, gender#13059, policy_number#13060, insured_name#13061, insured_gender#13062, insured_age#13063, insured_address#13064, insured_city#13065, insured_state#13066, insured_postal_code#13067, insured_country#13068, policy_start_date#13069, policy_end_date#13070, premium_amount#13071, deductible_amount#13072, coverage_limit#13073, policy_type#13074, insurance_company#13075, agent_name#13076, agent_email#13077, agent_phone#13078, ... 8 more fields]\n               +- Project [id#13119, first_name#13056, last_name#13057, email#13058, gender#13059, policy_number#13060, insured_name#13061, insured_gender#13062, insured_age#13063, insured_address#13064, insured_city#13065, insured_state#13066, insured_postal_code#13067, insured_country#13068, policy_start_date#13069, policy_end_date#13070, premium_amount#13071, deductible_amount#13072, coverage_limit#13073, policy_type#13074, insurance_company#13075, agent_name#13076, agent_email#13077, agent_phone#13078, ... 8 more fields]\n                  +- Filter RLIKE(id#13119, ^\\d+$)\n                     +- Project [regexp_extract(id#13055, \\d+, 0) AS id#13119, first_name#13056, last_name#13057, email#13058, gender#13059, policy_number#13060, insured_name#13061, insured_gender#13062, insured_age#13063, insured_address#13064, insured_city#13065, insured_state#13066, insured_postal_code#13067, insured_country#13068, policy_start_date#13069, policy_end_date#13070, premium_amount#13071, deductible_amount#13072, coverage_limit#13073, policy_type#13074, insurance_company#13075, agent_name#13076, agent_email#13077, agent_phone#13078, ... 8 more fields]\n                        +- Relation [id#13055,first_name#13056,last_name#13057,email#13058,gender#13059,policy_number#13060,insured_name#13061,insured_gender#13062,insured_age#13063,insured_address#13064,insured_city#13065,insured_state#13066,insured_postal_code#13067,insured_country#13068,policy_start_date#13069,policy_end_date#13070,premium_amount#13071,deductible_amount#13072,coverage_limit#13073,policy_type#13074,insurance_company#13075,agent_name#13076,agent_email#13077,agent_phone#13078,... 8 more fields] csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymssql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, regexp_extract\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Inicializa la sesión de Spark\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"DataCleaning\").getOrCreate()\n",
    "\n",
    "# Ruta del archivo cargado\n",
    "file_path = \"C:\\\\Users\\\\Usuario\\\\Documents\\\\WILLIAM\\\\DOCUMENTOS\\\\PERSONALES\\\\MONOKERA\\\\MOCK_DATA.csv\"\n",
    "\n",
    "# Leer el archivo CSV y asegurarse de que tiene encabezado y tipo de datos inferido\n",
    "data_3 = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Verificar los nombres de las columnas\n",
    "print(\"Column names: \", data_3.columns)\n",
    "\n",
    "# Limpiar la columna 'id' para extraer solo los números (si están en el formato esperado)\n",
    "data_3 = data_3.withColumn('id', regexp_extract(col('id'), r'\\d+', 0))\n",
    "\n",
    "# Filtrar filas donde 'id' no es nulo y contiene solo números\n",
    "data_3_cleaned = data_3.filter(col('id').rlike('^\\d+$'))\n",
    "\n",
    "# Verificar si las columnas de fecha existen y convertirlas al formato correcto\n",
    "date_columns = [\"claim_date\", \"payment_date\"]\n",
    "for col_name in date_columns:\n",
    "    if col_name in data_3_cleaned.columns:\n",
    "        data_3_cleaned = data_3_cleaned.withColumn(col_name, to_date(col(col_name), \"M/d/yyyy\"))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada\")\n",
    "\n",
    "# Manejo de valores nulos\n",
    "# Rellenar valores nulos con valores predeterminados\n",
    "data_cleaned = data_3_cleaned.fillna({\n",
    "    \"insured_age\": 0,\n",
    "    \"insured_gender\": \"Unknown\",\n",
    "    \"claim_status\": \"Unknown\",\n",
    "    \"payment_status\": \"Unknown\",\n",
    "    \"payment_method\": \"Unknown\"\n",
    "})\n",
    "\n",
    "# Convertir columnas numéricas relevantes a tipos adecuados\n",
    "numeric_columns = [\"claim_amount\", \"payment_amount\"]\n",
    "for col_name in numeric_columns:\n",
    "    if col_name in data_cleaned.columns:\n",
    "        data_cleaned = data_cleaned.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada para conversión a tipo numérico\")\n",
    "\n",
    "# Eliminar duplicados antes de insertar (especialmente para claves primarias)\n",
    "data_cleaned = data_cleaned.dropDuplicates()\n",
    "\n",
    "# Mostrar las primeras filas después de limpieza\n",
    "data_cleaned.show(5)\n",
    "\n",
    "# **CONEXIÓN A SQL SERVER CON pymssql Y ESCRITURA DE DATOS**\n",
    "\n",
    "# Configuración de la conexión a SQL Server\n",
    "server = \"LAPTOP-O07NV287\"  # Nombre del servidor\n",
    "database = \"monokera\"  # Nombre de la base de datos\n",
    "username = \"williamxlr\"  # Usuario\n",
    "password = \"Al3xW$1978\"  # Contraseña\n",
    "\n",
    "# Conectar a la base de datos usando pymssql\n",
    "conn = pymssql.connect(server=server, user=username, password=password, database=database)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# **1. Insertar en la tabla 'Policy'**\n",
    "policy_data = data_cleaned.select(\"policy_number\", \"policy_start_date\", \"policy_end_date\", \"policy_type\", \"insurance_company\")\n",
    "policy_data = policy_data.dropDuplicates([\"policy_number\"])\n",
    "\n",
    "for row in policy_data.collect():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Policy (policy_number, policy_start_date, policy_end_date, policy_type, insurance_company)\n",
    "        VALUES (%s, %s, %s, %s, %s)\n",
    "    \"\"\", (row['policy_number'], row['policy_start_date'], row['policy_end_date'], row['policy_type'], row['insurance_company']))\n",
    "\n",
    "# **2. Insertar en la tabla 'Insured'**\n",
    "insured_data = data_cleaned.select(\"insured_id\", \"policy_number\", \"insured_name\", \"insured_gender\", \"insured_age\",\n",
    "                                   \"insured_address\", \"insured_city\", \"insured_state\", \"insured_postal_code\", \"insured_country\")\n",
    "insured_data = insured_data.dropDuplicates([\"insured_id\"])\n",
    "\n",
    "for row in insured_data.collect():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Insured (insured_id, policy_number, insured_name, insured_gender, insured_age, insured_address, insured_city, insured_state, insured_postal_code, insured_country)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\", (row['insured_id'], row['policy_number'], row['insured_name'], row['insured_gender'], row['insured_age'],\n",
    "          row['insured_address'], row['insured_city'], row['insured_state'], row['insured_postal_code'], row['insured_country']))\n",
    "\n",
    "# **3. Insertar en la tabla 'Premium'**\n",
    "premium_data = data_cleaned.select(\"policy_number\", \"premium_amount\", \"deductible_amount\", \"coverage_limit\")\n",
    "premium_data = premium_data.dropDuplicates([\"policy_number\"])\n",
    "\n",
    "for row in premium_data.collect():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Premium (policy_number, premium_amount, deductible_amount, coverage_limit)\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "    \"\"\", (row['policy_number'], row['premium_amount'], row['deductible_amount'], row['coverage_limit']))\n",
    "\n",
    "# **4. Insertar en la tabla 'Payments'**\n",
    "payments_data = data_cleaned.select(\"policy_number\", \"payment_status\", \"payment_date\", \"payment_amount\", \"payment_method\")\n",
    "payments_data = payments_data.dropDuplicates([\"policy_number\", \"payment_date\"])\n",
    "\n",
    "for row in payments_data.collect():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Payments (policy_number, payment_status, payment_date, payment_amount, payment_method)\n",
    "        VALUES (%s, %s, %s, %s, %s)\n",
    "    \"\"\", (row['policy_number'], row['payment_status'], row['payment_date'], row['payment_amount'], row['payment_method']))\n",
    "\n",
    "# **5. Insertar en la tabla 'Claims'**\n",
    "claims_data = data_cleaned.select(\"policy_number\", \"claim_status\", \"claim_date\", \"claim_amount\", \"claim_description\")\n",
    "claims_data = claims_data.dropDuplicates([\"policy_number\", \"claim_date\"])\n",
    "\n",
    "for row in claims_data.collect():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Claims (policy_number, claim_status, claim_date, claim_amount, claim_description)\n",
    "        VALUES (%s, %s, %s, %s, %s)\n",
    "    \"\"\", (row['policy_number'], row['claim_status'], row['claim_date'], row['claim_amount'], row['claim_description']))\n",
    "\n",
    "# **6. Insertar en la tabla 'Agents'**\n",
    "agents_data = data_cleaned.select(\"policy_number\", \"agent_name\", \"agent_email\", \"agent_phone\")\n",
    "agents_data = agents_data.dropDuplicates([\"agent_email\"])\n",
    "\n",
    "for row in agents_data.collect():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Agents (policy_number, agent_name, agent_email, agent_phone)\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "    \"\"\", (row['policy_number'], row['agent_name'], row['agent_email'], row['agent_phone']))\n",
    "\n",
    "# Confirmar los cambios y cerrar la conexión\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:  ['id', 'first_name', 'last_name', 'email', 'gender', 'policy_number', 'insured_name', 'insured_gender', 'insured_age', 'insured_address', 'insured_city', 'insured_state', 'insured_postal_code', 'insured_country', 'policy_start_date', 'policy_end_date', 'premium_amount', 'deductible_amount', 'coverage_limit', 'policy_type', 'insurance_company', 'agent_name', 'agent_email', 'agent_phone', 'claim_status', 'claim_date', 'claim_amount', 'claim_description', 'payment_status', 'payment_date', 'payment_amount', 'payment_method']\n",
      "+---+----------+---------+--------------------+------+-------------+----------------+--------------+-----------+--------------------+------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "| id|first_name|last_name|               email|gender|policy_number|    insured_name|insured_gender|insured_age|     insured_address|insured_city|insured_state|insured_postal_code|insured_country|policy_start_date|policy_end_date|premium_amount|deductible_amount|coverage_limit|policy_type|insurance_company|      agent_name|         agent_email|      agent_phone|claim_status|claim_date|claim_amount|   claim_description|payment_status|payment_date|payment_amount|payment_method|\n",
      "+---+----------+---------+--------------------+------+-------------+----------------+--------------+-----------+--------------------+------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "|293|     Lorne|    Denne| ldenne84@smh.com.au|  Male|          293|     Lorne Denne|          Male|         33|28393 Goodland Cr...|   Davenport|         Iowa|              52804|  United States|       2023-06-19|     01/11/2024|       2251.36|           638.48|     351587.59|       Home|         Linkbuzz|     Lorne Denne|ldenne84@shop-pro.jp|+1 (563) 596-5970|    Approved|2022-10-26|     3695.83|Etiam vel augue. ...|       Pending|  2022-06-01|        447.76|         Check|\n",
      "|915|    Ralina|    Gabel|rgabelpe@vkontakt...|  Male|          915|    Ralina Gabel|        Female|          0|9844 Claremont Av...| Springfield|     Missouri|              65898|  United States|       2023-02-08|     08/19/2023|       5412.31|          3491.01|      551417.7|       Life|           Zoozzy|    Ralina Gabel|    rgabelpe@mlb.com|+1 (417) 562-6585|     Pending|2022-01-24|     8411.61|Aenean lectus. Pe...|          Paid|  2022-10-06|        748.59| Bank Transfer|\n",
      "|917|    Jessey|  Panting|jpantingpg@state....|Female|          917|  Jessey Panting|   Genderqueer|         58|    74170 Nelson Way|   Nashville|    Tennessee|              37220|  United States|       2023-08-22|     10/14/2023|       4016.36|           816.55|     329815.95|       Auto|            Quire|  Jessey Panting|jpantingpg@blogs.com|+1 (615) 257-3106|    Approved|2022-09-29|     4838.33|Sed ante. Vivamus...|       Pending|  2022-06-03|         698.4|         Check|\n",
      "|533|   Fremont| Baversor| fbaversores@com.com|Female|          533|Fremont Baversor|          Male|         19|       167 Lien Road|        Lima|         Ohio|              45807|  United States|       2023-10-23|     03/13/2023|       1484.06|          2885.32|      377748.2|     Health|             Yotz|Fremont Baversor|fbaversores@studi...|+1 (419) 509-1666|    Approved|2022-12-17|     7629.76|Quisque id justo ...|          Paid|  2022-09-12|        515.01| Bank Transfer|\n",
      "|677|    Cathie|Kingsford|ckingsfordis@usgs...|  Male|          677|Cathie Kingsford|        Female|         40|596 Loeprich Junc...|    Sarasota|      Florida|              34233|  United States|       2024-01-24|     12/03/2023|        5194.9|          4326.98|     994158.85|       Life|            Jaloo|Cathie Kingsford|ckingsfordis@yell...|+1 (941) 606-1916|    Approved|2022-06-17|     8904.59|Duis consequat du...|       Pending|  2022-05-16|         972.9|   Credit Card|\n",
      "+---+----------+---------+--------------------+------+-------------+----------------+--------------+-----------+--------------------+------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `insured_id` cannot be resolved. Did you mean one of the following? [`insured_age`, `insured_city`, `insured_name`, `insured_gender`, `insured_state`].;\n'Project ['insured_id, policy_number#13588, insured_name#13589, insured_gender#13779, insured_age#13780, insured_address#13592, insured_city#13593, insured_state#13594, insured_postal_code#13595, insured_country#13596]\n+- Deduplicate [agent_email#13605, claim_description#13610, insurance_company#13603, first_name#13584, insured_age#13780, email#13586, coverage_limit#13601, insured_name#13589, agent_name#13604, claim_date#13681, insured_city#13593, payment_date#13714, claim_status#13781, insured_country#13596, insured_address#13592, insured_state#13594, payment_method#13783, payment_status#13782, id#13647, policy_type#13602, last_name#13585, policy_end_date#13598, insured_gender#13779, policy_start_date#13597, ... 8 more fields]\n   +- Project [id#13647, first_name#13584, last_name#13585, email#13586, gender#13587, policy_number#13588, insured_name#13589, insured_gender#13779, insured_age#13780, insured_address#13592, insured_city#13593, insured_state#13594, insured_postal_code#13595, insured_country#13596, policy_start_date#13597, policy_end_date#13598, premium_amount#13599, deductible_amount#13600, coverage_limit#13601, policy_type#13602, insurance_company#13603, agent_name#13604, agent_email#13605, agent_phone#13606, ... 8 more fields]\n      +- Project [id#13647, first_name#13584, last_name#13585, email#13586, gender#13587, policy_number#13588, insured_name#13589, insured_gender#13779, insured_age#13780, insured_address#13592, insured_city#13593, insured_state#13594, insured_postal_code#13595, insured_country#13596, policy_start_date#13597, policy_end_date#13598, premium_amount#13599, deductible_amount#13600, coverage_limit#13601, policy_type#13602, insurance_company#13603, agent_name#13604, agent_email#13605, agent_phone#13606, ... 8 more fields]\n         +- Project [id#13647, first_name#13584, last_name#13585, email#13586, gender#13587, policy_number#13588, insured_name#13589, coalesce(insured_gender#13590, cast(Unknown as string)) AS insured_gender#13779, coalesce(insured_age#13591, cast(0 as string)) AS insured_age#13780, insured_address#13592, insured_city#13593, insured_state#13594, insured_postal_code#13595, insured_country#13596, policy_start_date#13597, policy_end_date#13598, premium_amount#13599, deductible_amount#13600, coverage_limit#13601, policy_type#13602, insurance_company#13603, agent_name#13604, agent_email#13605, agent_phone#13606, ... 8 more fields]\n            +- Project [id#13647, first_name#13584, last_name#13585, email#13586, gender#13587, policy_number#13588, insured_name#13589, insured_gender#13590, insured_age#13591, insured_address#13592, insured_city#13593, insured_state#13594, insured_postal_code#13595, insured_country#13596, policy_start_date#13597, policy_end_date#13598, premium_amount#13599, deductible_amount#13600, coverage_limit#13601, policy_type#13602, insurance_company#13603, agent_name#13604, agent_email#13605, agent_phone#13606, ... 8 more fields]\n               +- Project [id#13647, first_name#13584, last_name#13585, email#13586, gender#13587, policy_number#13588, insured_name#13589, insured_gender#13590, insured_age#13591, insured_address#13592, insured_city#13593, insured_state#13594, insured_postal_code#13595, insured_country#13596, policy_start_date#13597, policy_end_date#13598, premium_amount#13599, deductible_amount#13600, coverage_limit#13601, policy_type#13602, insurance_company#13603, agent_name#13604, agent_email#13605, agent_phone#13606, ... 8 more fields]\n                  +- Filter RLIKE(id#13647, ^\\d+$)\n                     +- Project [regexp_extract(id#13583, \\d+, 0) AS id#13647, first_name#13584, last_name#13585, email#13586, gender#13587, policy_number#13588, insured_name#13589, insured_gender#13590, insured_age#13591, insured_address#13592, insured_city#13593, insured_state#13594, insured_postal_code#13595, insured_country#13596, policy_start_date#13597, policy_end_date#13598, premium_amount#13599, deductible_amount#13600, coverage_limit#13601, policy_type#13602, insurance_company#13603, agent_name#13604, agent_email#13605, agent_phone#13606, ... 8 more fields]\n                        +- Relation [id#13583,first_name#13584,last_name#13585,email#13586,gender#13587,policy_number#13588,insured_name#13589,insured_gender#13590,insured_age#13591,insured_address#13592,insured_city#13593,insured_state#13594,insured_postal_code#13595,insured_country#13596,policy_start_date#13597,policy_end_date#13598,premium_amount#13599,deductible_amount#13600,coverage_limit#13601,policy_type#13602,insurance_company#13603,agent_name#13604,agent_email#13605,agent_phone#13606,... 8 more fields] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 80\u001b[0m\n\u001b[0;32m     74\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124m        INSERT INTO Policy (policy_number, policy_start_date, policy_end_date, policy_type, insurance_company)\u001b[39m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124m        VALUES (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m, (row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy_number\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy_start_date\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy_end_date\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy_type\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minsurance_company\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# **2. Insertar en la tabla 'Insured'**\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m insured_data \u001b[38;5;241m=\u001b[39m \u001b[43mdata_cleaned\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minsured_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpolicy_number\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minsured_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minsured_gender\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minsured_age\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minsured_address\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minsured_city\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minsured_state\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minsured_postal_code\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minsured_country\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m insured_data \u001b[38;5;241m=\u001b[39m insured_data\u001b[38;5;241m.\u001b[39mdropDuplicates([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsured_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m insured_data\u001b[38;5;241m.\u001b[39mcollect():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\dataframe.py:3229\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   3184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   3185\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   3186\u001b[0m \n\u001b[0;32m   3187\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3227\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[0;32m   3228\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3229\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `insured_id` cannot be resolved. Did you mean one of the following? [`insured_age`, `insured_city`, `insured_name`, `insured_gender`, `insured_state`].;\n'Project ['insured_id, policy_number#13588, insured_name#13589, insured_gender#13779, insured_age#13780, insured_address#13592, insured_city#13593, insured_state#13594, insured_postal_code#13595, insured_country#13596]\n+- Deduplicate [agent_email#13605, claim_description#13610, insurance_company#13603, first_name#13584, insured_age#13780, email#13586, coverage_limit#13601, insured_name#13589, agent_name#13604, claim_date#13681, insured_city#13593, payment_date#13714, claim_status#13781, insured_country#13596, insured_address#13592, insured_state#13594, payment_method#13783, payment_status#13782, id#13647, policy_type#13602, last_name#13585, policy_end_date#13598, insured_gender#13779, policy_start_date#13597, ... 8 more fields]\n   +- Project [id#13647, first_name#13584, last_name#13585, email#13586, gender#13587, policy_number#13588, insured_name#13589, insured_gender#13779, insured_age#13780, insured_address#13592, insured_city#13593, insured_state#13594, insured_postal_code#13595, insured_country#13596, policy_start_date#13597, policy_end_date#13598, premium_amount#13599, deductible_amount#13600, coverage_limit#13601, policy_type#13602, insurance_company#13603, agent_name#13604, agent_email#13605, agent_phone#13606, ... 8 more fields]\n      +- Project [id#13647, first_name#13584, last_name#13585, email#13586, gender#13587, policy_number#13588, insured_name#13589, insured_gender#13779, insured_age#13780, insured_address#13592, insured_city#13593, insured_state#13594, insured_postal_code#13595, insured_country#13596, policy_start_date#13597, policy_end_date#13598, premium_amount#13599, deductible_amount#13600, coverage_limit#13601, policy_type#13602, insurance_company#13603, agent_name#13604, agent_email#13605, agent_phone#13606, ... 8 more fields]\n         +- Project [id#13647, first_name#13584, last_name#13585, email#13586, gender#13587, policy_number#13588, insured_name#13589, coalesce(insured_gender#13590, cast(Unknown as string)) AS insured_gender#13779, coalesce(insured_age#13591, cast(0 as string)) AS insured_age#13780, insured_address#13592, insured_city#13593, insured_state#13594, insured_postal_code#13595, insured_country#13596, policy_start_date#13597, policy_end_date#13598, premium_amount#13599, deductible_amount#13600, coverage_limit#13601, policy_type#13602, insurance_company#13603, agent_name#13604, agent_email#13605, agent_phone#13606, ... 8 more fields]\n            +- Project [id#13647, first_name#13584, last_name#13585, email#13586, gender#13587, policy_number#13588, insured_name#13589, insured_gender#13590, insured_age#13591, insured_address#13592, insured_city#13593, insured_state#13594, insured_postal_code#13595, insured_country#13596, policy_start_date#13597, policy_end_date#13598, premium_amount#13599, deductible_amount#13600, coverage_limit#13601, policy_type#13602, insurance_company#13603, agent_name#13604, agent_email#13605, agent_phone#13606, ... 8 more fields]\n               +- Project [id#13647, first_name#13584, last_name#13585, email#13586, gender#13587, policy_number#13588, insured_name#13589, insured_gender#13590, insured_age#13591, insured_address#13592, insured_city#13593, insured_state#13594, insured_postal_code#13595, insured_country#13596, policy_start_date#13597, policy_end_date#13598, premium_amount#13599, deductible_amount#13600, coverage_limit#13601, policy_type#13602, insurance_company#13603, agent_name#13604, agent_email#13605, agent_phone#13606, ... 8 more fields]\n                  +- Filter RLIKE(id#13647, ^\\d+$)\n                     +- Project [regexp_extract(id#13583, \\d+, 0) AS id#13647, first_name#13584, last_name#13585, email#13586, gender#13587, policy_number#13588, insured_name#13589, insured_gender#13590, insured_age#13591, insured_address#13592, insured_city#13593, insured_state#13594, insured_postal_code#13595, insured_country#13596, policy_start_date#13597, policy_end_date#13598, premium_amount#13599, deductible_amount#13600, coverage_limit#13601, policy_type#13602, insurance_company#13603, agent_name#13604, agent_email#13605, agent_phone#13606, ... 8 more fields]\n                        +- Relation [id#13583,first_name#13584,last_name#13585,email#13586,gender#13587,policy_number#13588,insured_name#13589,insured_gender#13590,insured_age#13591,insured_address#13592,insured_city#13593,insured_state#13594,insured_postal_code#13595,insured_country#13596,policy_start_date#13597,policy_end_date#13598,premium_amount#13599,deductible_amount#13600,coverage_limit#13601,policy_type#13602,insurance_company#13603,agent_name#13604,agent_email#13605,agent_phone#13606,... 8 more fields] csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymssql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, regexp_extract\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Inicializa la sesión de Spark\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"DataCleaning\").getOrCreate()\n",
    "\n",
    "# Ruta del archivo cargado\n",
    "file_path = \"C:\\\\Users\\\\Usuario\\\\Documents\\\\WILLIAM\\\\DOCUMENTOS\\\\PERSONALES\\\\MONOKERA\\\\MOCK_DATA.csv\"\n",
    "\n",
    "# Leer el archivo CSV y asegurarse de que tiene encabezado y tipo de datos inferido\n",
    "data_3 = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Verificar los nombres de las columnas\n",
    "print(\"Column names: \", data_3.columns)\n",
    "\n",
    "# Limpiar la columna 'id' para extraer solo los números (si están en el formato esperado)\n",
    "data_3 = data_3.withColumn('id', regexp_extract(col('id'), r'\\d+', 0))\n",
    "\n",
    "# Filtrar filas donde 'id' no es nulo y contiene solo números\n",
    "data_3_cleaned = data_3.filter(col('id').rlike('^\\d+$'))\n",
    "\n",
    "# Verificar si las columnas de fecha existen y convertirlas al formato correcto\n",
    "date_columns = [\"claim_date\", \"payment_date\"]\n",
    "for col_name in date_columns:\n",
    "    if col_name in data_3_cleaned.columns:\n",
    "        data_3_cleaned = data_3_cleaned.withColumn(col_name, to_date(col(col_name), \"M/d/yyyy\"))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada\")\n",
    "\n",
    "# Manejo de valores nulos\n",
    "# Rellenar valores nulos con valores predeterminados\n",
    "data_cleaned = data_3_cleaned.fillna({\n",
    "    \"insured_age\": 0,\n",
    "    \"insured_gender\": \"Unknown\",\n",
    "    \"claim_status\": \"Unknown\",\n",
    "    \"payment_status\": \"Unknown\",\n",
    "    \"payment_method\": \"Unknown\"\n",
    "})\n",
    "\n",
    "# Convertir columnas numéricas relevantes a tipos adecuados\n",
    "numeric_columns = [\"claim_amount\", \"payment_amount\"]\n",
    "for col_name in numeric_columns:\n",
    "    if col_name in data_cleaned.columns:\n",
    "        data_cleaned = data_cleaned.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada para conversión a tipo numérico\")\n",
    "\n",
    "# Eliminar duplicados antes de insertar (especialmente para claves primarias)\n",
    "data_cleaned = data_cleaned.dropDuplicates()\n",
    "\n",
    "# Mostrar las primeras filas después de limpieza\n",
    "data_cleaned.show(5)\n",
    "\n",
    "# **CONEXIÓN A SQL SERVER CON pymssql Y ESCRITURA DE DATOS**\n",
    "\n",
    "# Configuración de la conexión a SQL Server\n",
    "server = \"LAPTOP-O07NV287\"  # Nombre del servidor\n",
    "database = \"monokera\"  # Nombre de la base de datos\n",
    "username = \"williamxlr\"  # Usuario\n",
    "password = \"Al3xW$1978\"  # Contraseña\n",
    "\n",
    "# Conectar a la base de datos usando pymssql\n",
    "conn = pymssql.connect(server=server, user=username, password=password, database=database)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# **1. Insertar en la tabla 'Policy'**\n",
    "policy_data = data_cleaned.select(\"policy_number\", \"policy_start_date\", \"policy_end_date\", \"policy_type\", \"insurance_company\")\n",
    "policy_data = policy_data.dropDuplicates([\"policy_number\"])\n",
    "\n",
    "for row in policy_data.collect():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Policy (policy_number, policy_start_date, policy_end_date, policy_type, insurance_company)\n",
    "        VALUES (%s, %s, %s, %s, %s)\n",
    "    \"\"\", (row['policy_number'], row['policy_start_date'], row['policy_end_date'], row['policy_type'], row['insurance_company']))\n",
    "\n",
    "# **2. Insertar en la tabla 'Insured'**\n",
    "insured_data = data_cleaned.select(\"insured_id\", \"policy_number\", \"insured_name\", \"insured_gender\", \"insured_age\",\n",
    "                                   \"insured_address\", \"insured_city\", \"insured_state\", \"insured_postal_code\", \"insured_country\")\n",
    "insured_data = insured_data.dropDuplicates([\"insured_id\"])\n",
    "\n",
    "for row in insured_data.collect():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Insured (insured_id, policy_number, insured_name, insured_gender, insured_age, insured_address, insured_city, insured_state, insured_postal_code, insured_country)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\", (row['insured_id'], row['policy_number'], row['insured_name'], row['insured_gender'], row['insured_age'],\n",
    "          row['insured_address'], row['insured_city'], row['insured_state'], row['insured_postal_code'], row['insured_country']))\n",
    "\n",
    "# **3. Insertar en la tabla 'Premium'**\n",
    "premium_data = data_cleaned.select(\"policy_number\", \"premium_amount\", \"deductible_amount\", \"coverage_limit\")\n",
    "premium_data = premium_data.dropDuplicates([\"policy_number\"])\n",
    "\n",
    "for row in premium_data.collect():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Premium (policy_number, premium_amount, deductible_amount, coverage_limit)\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "    \"\"\", (row['policy_number'], row['premium_amount'], row['deductible_amount'], row['coverage_limit']))\n",
    "\n",
    "# **4. Insertar en la tabla 'Payments'**\n",
    "payments_data = data_cleaned.select(\"policy_number\", \"payment_status\", \"payment_date\", \"payment_amount\", \"payment_method\")\n",
    "payments_data = payments_data.dropDuplicates([\"policy_number\", \"payment_date\"])\n",
    "\n",
    "for row in payments_data.collect():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Payments (policy_number, payment_status, payment_date, payment_amount, payment_method)\n",
    "        VALUES (%s, %s, %s, %s, %s)\n",
    "    \"\"\", (row['policy_number'], row['payment_status'], row['payment_date'], row['payment_amount'], row['payment_method']))\n",
    "\n",
    "# **5. Insertar en la tabla 'Claims'**\n",
    "claims_data = data_cleaned.select(\"policy_number\", \"claim_status\", \"claim_date\", \"claim_amount\", \"claim_description\")\n",
    "claims_data = claims_data.dropDuplicates([\"policy_number\", \"claim_date\"])\n",
    "\n",
    "for row in claims_data.collect():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Claims (policy_number, claim_status, claim_date, claim_amount, claim_description)\n",
    "        VALUES (%s, %s, %s, %s, %s)\n",
    "    \"\"\", (row['policy_number'], row['claim_status'], row['claim_date'], row['claim_amount'], row['claim_description']))\n",
    "\n",
    "# **6. Insertar en la tabla 'Agents'**\n",
    "agents_data = data_cleaned.select(\"policy_number\", \"agent_name\", \"agent_email\", \"agent_phone\")\n",
    "agents_data = agents_data.dropDuplicates([\"agent_email\"])\n",
    "\n",
    "for row in agents_data.collect():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Agents (policy_number, agent_name, agent_email, agent_phone)\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "    \"\"\", (row['policy_number'], row['agent_name'], row['agent_email'], row['agent_phone']))\n",
    "\n",
    "# Confirmar los cambios y cerrar la conexión\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:  ['id', 'first_name', 'last_name', 'email', 'gender', 'policy_number', 'insured_name', 'insured_gender', 'insured_age', 'insured_address', 'insured_city', 'insured_state', 'insured_postal_code', 'insured_country', 'policy_start_date', 'policy_end_date', 'premium_amount', 'deductible_amount', 'coverage_limit', 'policy_type', 'insurance_company', 'agent_name', 'agent_email', 'agent_phone', 'claim_status', 'claim_date', 'claim_amount', 'claim_description', 'payment_status', 'payment_date', 'payment_amount', 'payment_method']\n",
      "+---+----------+-----------+--------------------+------+-------------+-----------------+--------------+-----------+--------------------+-------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+-----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "| id|first_name|  last_name|               email|gender|policy_number|     insured_name|insured_gender|insured_age|     insured_address| insured_city|insured_state|insured_postal_code|insured_country|policy_start_date|policy_end_date|premium_amount|deductible_amount|coverage_limit|policy_type|insurance_company|       agent_name|         agent_email|      agent_phone|claim_status|claim_date|claim_amount|   claim_description|payment_status|payment_date|payment_amount|payment_method|\n",
      "+---+----------+-----------+--------------------+------+-------------+-----------------+--------------+-----------+--------------------+-------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+-----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "|  1|     Bessy|Tomaszynski|btomaszynski0@ici...|  Male|            1|Bessy Tomaszynski|       Unknown|         63|   1 Northland Trail|      Suffolk|     Virginia|               NULL|           NULL|       2023-06-05|     03/13/2023|       7506.95|           263.51|     851969.14|       Home|            Jazzy|Bessy Tomaszynski|btomaszynski0@ale...|+1 (757) 646-3536|    Approved|2022-12-11|      7589.6|Duis bibendum, fe...|       Unknown|        NULL|          NULL|       Unknown|\n",
      "|  2|      Cher|     Gjerde|  cgjerde1@hexun.com|  NULL|            2|      Cher Gjerde|        Female|         19|     98989 Basil Way|    Pensacola|      Florida|              32526|           NULL|       2023-11-09|     01/24/2023|       1856.49|          4149.44|     138025.78|       Life|      Brainlounge|      Cher Gjerde|  cgjerde1@fotki.com|+1 (850) 302-7531|      Denied|2022-08-20|      5777.0|Quisque id justo ...|       Unknown|        NULL|          NULL|       Unknown|\n",
      "|  3|  Felicity|     Lowman| flowman2@flavors.me|  Male|            3|  Felicity Lowman|        Female|         43|       2 Derek Place|Pompano Beach|      Florida|              33075|           NULL|       2023-07-21|     05/05/2023|       9799.03|           598.63|     219401.29|       Life|         Realfire|  Felicity Lowman|flowman2@wikispac...|+1 (754) 879-3501|     Pending|2022-05-04|     1684.43|In sagittis dui v...|       Unknown|        NULL|          NULL|       Unknown|\n",
      "|  4|       Ive|    Jervois|   ijervois3@epa.gov|  Male|            4|      Ive Jervois|          Male|         60|889 Twin Pines Court|       Albany|     New York|              12242|  United States|       2023-08-08|     07/18/2023|        290.53|          4602.64|     669242.87|       Life|      Babblestorm|      Ive Jervois|ijervois3@ebay.co.uk|             NULL|    Approved|2022-08-09|     8160.13|In hac habitasse ...|       Unknown|        NULL|          NULL|       Unknown|\n",
      "|  5|    Gretal|   Bauckham|gbauckham4@slides...|Female|            5|  Gretal Bauckham|        Female|         29|39745 Clarendon Hill|      Phoenix|      Arizona|               NULL|  United States|       2023-12-24|     03/07/2023|        307.87|          1618.01|     647041.09|       Home|          Tagtune|  Gretal Bauckham|   gbauckham4@goo.gl|+1 (480) 648-4238|    Approved|2022-06-08|     9694.67|Praesent id massa...|       Unknown|        NULL|          NULL|       Unknown|\n",
      "+---+----------+-----------+--------------------+------+-------------+-----------------+--------------+-----------+--------------------+-------------+-------------+-------------------+---------------+-----------------+---------------+--------------+-----------------+--------------+-----------+-----------------+-----------------+--------------------+-----------------+------------+----------+------------+--------------------+--------------+------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1498.jdbc.\n: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\r\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:766)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 68\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(col \u001b[38;5;129;01min\u001b[39;00m data_cleaned\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_number\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_start_date\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_end_date\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsurance_company\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m     67\u001b[0m     policy_data \u001b[38;5;241m=\u001b[39m data_cleaned\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_number\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_start_date\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_end_date\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsurance_company\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m     \u001b[43mpolicy_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjdbc_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPolicy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproperties\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlgunas columnas necesarias no existen para la tabla \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\readwriter.py:1984\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[1;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[0;32m   1982\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[0;32m   1983\u001b[0m     jprop\u001b[38;5;241m.\u001b[39msetProperty(k, properties[k])\n\u001b[1;32m-> 1984\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1498.jdbc.\n: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\r\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:766)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, regexp_extract\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Inicializa la sesión de Spark\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"DataCleaning\").getOrCreate()\n",
    "\n",
    "# Ruta del archivo cargado\n",
    "file_path = \"C:\\\\Users\\\\Usuario\\\\Documents\\\\WILLIAM\\\\DOCUMENTOS\\\\PERSONALES\\\\MONOKERA\\\\MOCK_DATA.csv\"\n",
    "\n",
    "# Leer el archivo CSV y asegurarse de que tiene encabezado y tipo de datos inferido\n",
    "data_3 = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Verificar los nombres de las columnas\n",
    "print(\"Column names: \", data_3.columns)\n",
    "\n",
    "# Limpiar la columna 'id' para extraer solo los números (si están en el formato esperado)\n",
    "# Usamos expresiones regulares para extraer solo la parte numérica\n",
    "data_3 = data_3.withColumn('id', regexp_extract(col('id'), r'\\d+', 0))\n",
    "\n",
    "# Filtrar filas donde 'id' no es nulo y contiene solo números\n",
    "data_3_cleaned = data_3.filter(col('id').rlike('^\\d+$'))\n",
    "\n",
    "# Verificar si las columnas de fecha existen y convertirlas al formato correcto\n",
    "date_columns = [\"claim_date\", \"payment_date\"]\n",
    "for col_name in date_columns:\n",
    "    if col_name in data_3_cleaned.columns:\n",
    "        data_3_cleaned = data_3_cleaned.withColumn(col_name, to_date(col(col_name), \"M/d/yyyy\"))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada\")\n",
    "\n",
    "# Manejo de valores nulos\n",
    "# Rellenar valores nulos con valores predeterminados\n",
    "data_cleaned = data_3_cleaned.fillna({\n",
    "    \"insured_age\": 0,\n",
    "    \"insured_gender\": \"Unknown\",\n",
    "    \"claim_status\": \"Unknown\",\n",
    "    \"payment_status\": \"Unknown\",\n",
    "    \"payment_method\": \"Unknown\"\n",
    "})\n",
    "\n",
    "# Convertir columnas numéricas relevantes a tipos adecuados\n",
    "numeric_columns = [\"claim_amount\", \"payment_amount\"]\n",
    "for col_name in numeric_columns:\n",
    "    if col_name in data_cleaned.columns:\n",
    "        data_cleaned = data_cleaned.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada para conversión a tipo numérico\")\n",
    "\n",
    "# Mostrar las primeras filas después de limpieza\n",
    "data_cleaned.show(5)\n",
    "\n",
    "# **CONEXIÓN A SQL SERVER Y ESCRITURA DE DATOS**\n",
    "\n",
    "# Configuración de la conexión a SQL Server\n",
    "jdbc_url = \"jdbc:sqlserver://LAPTOP-O07NV287:1433;databaseName=monokera\"\n",
    "properties = {\n",
    "    \"user\": \"williamxlr\",\n",
    "    \"password\": \"Al3xW$1978\",\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "# **1. Insertar en la tabla 'Policy'**\n",
    "# Verificar si las columnas existen\n",
    "if all(col in data_cleaned.columns for col in [\"policy_number\", \"policy_start_date\", \"policy_end_date\", \"policy_type\", \"insurance_company\"]):\n",
    "    policy_data = data_cleaned.select(\"policy_number\", \"policy_start_date\", \"policy_end_date\", \"policy_type\", \"insurance_company\")\n",
    "    policy_data.write.jdbc(url=jdbc_url, table=\"Policy\", mode=\"append\", properties=properties)\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Policy'\")\n",
    "\n",
    "# **2. Insertar en la tabla 'Insured'**\n",
    "# Verificar si las columnas existen\n",
    "if all(col in data_cleaned.columns for col in [\"insured_id\", \"policy_number\", \"insured_name\", \"insured_gender\", \"insured_age\",\n",
    "                                               \"insured_address\", \"insured_city\", \"insured_state\", \"insured_postal_code\", \"insured_country\"]):\n",
    "    insured_data = data_cleaned.select(\"insured_id\", \"policy_number\", \"insured_name\", \"insured_gender\", \"insured_age\",\n",
    "                                       \"insured_address\", \"insured_city\", \"insured_state\", \"insured_postal_code\", \"insured_country\")\n",
    "    insured_data.write.jdbc(url=jdbc_url, table=\"Insured\", mode=\"append\", properties=properties)\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Insured'\")\n",
    "\n",
    "# **3. Insertar en la tabla 'Premium'**\n",
    "# Verificar si las columnas existen\n",
    "if all(col in data_cleaned.columns for col in [\"policy_number\", \"premium_amount\", \"deductible_amount\", \"coverage_limit\"]):\n",
    "    premium_data = data_cleaned.select(\"policy_number\", \"premium_amount\", \"deductible_amount\", \"coverage_limit\")\n",
    "    premium_data.write.jdbc(url=jdbc_url, table=\"Premium\", mode=\"append\", properties=properties)\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Premium'\")\n",
    "\n",
    "# **4. Insertar en la tabla 'Payments'**\n",
    "# Verificar si las columnas existen\n",
    "if all(col in data_cleaned.columns for col in [\"policy_number\", \"payment_status\", \"payment_date\", \"payment_amount\", \"payment_method\"]):\n",
    "    payments_data = data_cleaned.select(\"policy_number\", \"payment_status\", \"payment_date\", \"payment_amount\", \"payment_method\")\n",
    "    payments_data.write.jdbc(url=jdbc_url, table=\"Payments\", mode=\"append\", properties=properties)\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Payments'\")\n",
    "\n",
    "# **5. Insertar en la tabla 'Claims'**\n",
    "# Verificar si las columnas existen\n",
    "if all(col in data_cleaned.columns for col in [\"policy_number\", \"claim_status\", \"claim_date\", \"claim_amount\", \"claim_description\"]):\n",
    "    claims_data = data_cleaned.select(\"policy_number\", \"claim_status\", \"claim_date\", \"claim_amount\", \"claim_description\")\n",
    "    claims_data.write.jdbc(url=jdbc_url, table=\"Claims\", mode=\"append\", properties=properties)\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Claims'\")\n",
    "\n",
    "# **6. Insertar en la tabla 'Agents'**\n",
    "# Verificar si las columnas existen\n",
    "if all(col in data_cleaned.columns for col in [\"policy_number\", \"agent_name\", \"agent_email\", \"agent_phone\"]):\n",
    "    agents_data = data_cleaned.select(\"policy_number\", \"agent_name\", \"agent_email\", \"agent_phone\")\n",
    "    agents_data.write.jdbc(url=jdbc_url, table=\"Agents\", mode=\"append\", properties=properties)\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Agents'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:  ['id', 'first_name', 'last_name', 'email', 'gender', 'policy_number', 'insured_name', 'insured_gender', 'insured_age', 'insured_address', 'insured_city', 'insured_state', 'insured_postal_code', 'insured_country', 'policy_start_date', 'policy_end_date', 'premium_amount', 'deductible_amount', 'coverage_limit', 'policy_type', 'insurance_company', 'agent_name', 'agent_email', 'agent_phone', 'claim_status', 'claim_date', 'claim_amount', 'claim_description', 'payment_status', 'payment_date', 'payment_amount', 'payment_method']\n",
      "Algunas columnas necesarias no existen para la tabla 'Insured'\n"
     ]
    },
    {
     "ename": "ProgrammingError",
     "evalue": "(207, b\"Invalid column name 'nan'.DB-Lib error message 20018, severity 16:\\nGeneral SQL Server error: Check messages from the SQL Server\\n\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMSSQLDatabaseException\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32msrc\\\\pymssql\\\\_pymssql.pyx:447\u001b[0m, in \u001b[0;36mpymssql._pymssql.Cursor.execute\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_mssql.pyx:1125\u001b[0m, in \u001b[0;36mpymssql._mssql.MSSQLConnection.execute_query\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_mssql.pyx:1156\u001b[0m, in \u001b[0;36mpymssql._mssql.MSSQLConnection.execute_query\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_mssql.pyx:1289\u001b[0m, in \u001b[0;36mpymssql._mssql.MSSQLConnection.format_and_run_query\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_mssql.pyx:1852\u001b[0m, in \u001b[0;36mpymssql._mssql.check_cancel_and_raise\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_mssql.pyx:1898\u001b[0m, in \u001b[0;36mpymssql._mssql.raise_MSSQLDatabaseException\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMSSQLDatabaseException\u001b[0m: (207, b\"Invalid column name 'nan'.DB-Lib error message 20018, severity 16:\\nGeneral SQL Server error: Check messages from the SQL Server\\n\")",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 108\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(col \u001b[38;5;129;01min\u001b[39;00m data_cleaned_pd\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_number\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpremium_amount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeductible_amount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoverage_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m data_cleaned_pd\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m--> 108\u001b[0m         \u001b[43minsert_if_not_exists\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPremium\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpolicy_number\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpremium_amount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeductible_amount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoverage_limit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpolicy_number\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpremium_amount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdeductible_amount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcoverage_limit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlgunas columnas necesarias no existen para la tabla \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPremium\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[50], line 75\u001b[0m, in \u001b[0;36minsert_if_not_exists\u001b[1;34m(table, columns, values)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m# Si no existe, insertar el registro\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     insert_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINSERT INTO \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) VALUES (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(values))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 75\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43minsert_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     conn\u001b[38;5;241m.\u001b[39mcommit()\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_pymssql.pyx:462\u001b[0m, in \u001b[0;36mpymssql._pymssql.Cursor.execute\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mProgrammingError\u001b[0m: (207, b\"Invalid column name 'nan'.DB-Lib error message 20018, severity 16:\\nGeneral SQL Server error: Check messages from the SQL Server\\n\")"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymssql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, regexp_extract\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Inicializa la sesión de Spark\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"DataCleaning\").getOrCreate()\n",
    "\n",
    "# Ruta del archivo cargado\n",
    "file_path = \"C:\\\\Users\\\\Usuario\\\\Documents\\\\WILLIAM\\\\DOCUMENTOS\\\\PERSONALES\\\\MONOKERA\\\\MOCK_DATA.csv\"\n",
    "\n",
    "# Leer el archivo CSV y asegurarse de que tiene encabezado y tipo de datos inferido\n",
    "data_3 = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Verificar los nombres de las columnas\n",
    "print(\"Column names: \", data_3.columns)\n",
    "\n",
    "# Limpiar la columna 'id' para extraer solo los números (si están en el formato esperado)\n",
    "data_3 = data_3.withColumn('id', regexp_extract(col('id'), r'\\d+', 0))\n",
    "\n",
    "# Filtrar filas donde 'id' no es nulo y contiene solo números\n",
    "data_3_cleaned = data_3.filter(col('id').rlike('^\\d+$'))\n",
    "\n",
    "# Verificar si las columnas de fecha existen y convertirlas al formato correcto\n",
    "date_columns = [\"claim_date\", \"payment_date\"]\n",
    "for col_name in date_columns:\n",
    "    if col_name in data_3_cleaned.columns:\n",
    "        data_3_cleaned = data_3_cleaned.withColumn(col_name, to_date(col(col_name), \"M/d/yyyy\"))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada\")\n",
    "\n",
    "# Manejo de valores nulos\n",
    "data_cleaned = data_3_cleaned.fillna({\n",
    "    \"insured_age\": 0,\n",
    "    \"insured_gender\": \"Unknown\",\n",
    "    \"claim_status\": \"Unknown\",\n",
    "    \"payment_status\": \"Unknown\",\n",
    "    \"payment_method\": \"Unknown\"\n",
    "})\n",
    "\n",
    "# Convertir columnas numéricas relevantes a tipos adecuados\n",
    "numeric_columns = [\"claim_amount\", \"payment_amount\"]\n",
    "for col_name in numeric_columns:\n",
    "    if col_name in data_cleaned.columns:\n",
    "        data_cleaned = data_cleaned.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada para conversión a tipo numérico\")\n",
    "\n",
    "# Convertir el DataFrame de Spark a Pandas para usar pymssql\n",
    "data_cleaned_pd = data_cleaned.toPandas()\n",
    "\n",
    "# **CONEXIÓN A SQL SERVER Y ESCRITURA DE DATOS CON pymssql**\n",
    "\n",
    "# Establecer la conexión con SQL Server\n",
    "conn = pymssql.connect(\n",
    "    server=\"LAPTOP-O07NV287\",  # Dirección del servidor\n",
    "    user=\"williamxlr\",         # Nombre de usuario\n",
    "    password=\"Al3xW$1978\",     # Contraseña\n",
    "    database=\"monokera\"        # Nombre de la base de datos\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Función para insertar solo si el registro no existe\n",
    "def insert_if_not_exists(table, columns, values):\n",
    "    # Crear consulta para verificar si el registro ya existe\n",
    "    select_query = f\"SELECT COUNT(*) FROM {table} WHERE {columns[0]} = %s\"\n",
    "    cursor.execute(select_query, (values[0],))\n",
    "    result = cursor.fetchone()\n",
    "    \n",
    "    if result[0] == 0:\n",
    "        # Si no existe, insertar el registro\n",
    "        insert_query = f\"INSERT INTO {table} ({', '.join(columns)}) VALUES ({', '.join(['%s'] * len(values))})\"\n",
    "        cursor.execute(insert_query, values)\n",
    "        conn.commit()\n",
    "    else:\n",
    "        print(f\"Registro con {columns[0]} = {values[0]} ya existe en la tabla {table}.\")\n",
    "\n",
    "# **1. Insertar en la tabla 'Policy'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"policy_number\", \"policy_start_date\", \"policy_end_date\", \"policy_type\", \"insurance_company\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        insert_if_not_exists(\n",
    "            \"Policy\",\n",
    "            [\"policy_number\", \"policy_start_date\", \"policy_end_date\", \"policy_type\", \"insurance_company\"],\n",
    "            (row['policy_number'], row['policy_start_date'], row['policy_end_date'], row['policy_type'], row['insurance_company'])\n",
    "        )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Policy'\")\n",
    "\n",
    "# **2. Insertar en la tabla 'Insured'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"insured_id\", \"policy_number\", \"insured_name\", \"insured_gender\", \"insured_age\",\n",
    "                                                 \"insured_address\", \"insured_city\", \"insured_state\", \"insured_postal_code\", \"insured_country\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        insert_if_not_exists(\n",
    "            \"Insured\",\n",
    "            [\"insured_id\", \"policy_number\", \"insured_name\", \"insured_gender\", \"insured_age\", \n",
    "             \"insured_address\", \"insured_city\", \"insured_state\", \"insured_postal_code\", \"insured_country\"],\n",
    "            (row['insured_id'], row['policy_number'], row['insured_name'], row['insured_gender'], row['insured_age'],\n",
    "             row['insured_address'], row['insured_city'], row['insured_state'], row['insured_postal_code'], row['insured_country'])\n",
    "        )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Insured'\")\n",
    "\n",
    "# **3. Insertar en la tabla 'Premium'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"policy_number\", \"premium_amount\", \"deductible_amount\", \"coverage_limit\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        insert_if_not_exists(\n",
    "            \"Premium\",\n",
    "            [\"policy_number\", \"premium_amount\", \"deductible_amount\", \"coverage_limit\"],\n",
    "            (row['policy_number'], row['premium_amount'], row['deductible_amount'], row['coverage_limit'])\n",
    "        )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Premium'\")\n",
    "\n",
    "# **4. Insertar en la tabla 'Payments'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"policy_number\", \"payment_status\", \"payment_date\", \"payment_amount\", \"payment_method\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        insert_if_not_exists(\n",
    "            \"Payments\",\n",
    "            [\"policy_number\", \"payment_status\", \"payment_date\", \"payment_amount\", \"payment_method\"],\n",
    "            (row['policy_number'], row['payment_status'], row['payment_date'], row['payment_amount'], row['payment_method'])\n",
    "        )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Payments'\")\n",
    "\n",
    "# **5. Insertar en la tabla 'Claims'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"policy_number\", \"claim_status\", \"claim_date\", \"claim_amount\", \"claim_description\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        insert_if_not_exists(\n",
    "            \"Claims\",\n",
    "            [\"policy_number\", \"claim_status\", \"claim_date\", \"claim_amount\", \"claim_description\"],\n",
    "            (row['policy_number'], row['claim_status'], row['claim_date'], row['claim_amount'], row['claim_description'])\n",
    "        )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Claims'\")\n",
    "\n",
    "# **6. Insertar en la tabla 'Agents'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"policy_number\", \"agent_name\", \"agent_email\", \"agent_phone\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        insert_if_not_exists(\n",
    "            \"Agents\",\n",
    "            [\"policy_number\", \"agent_name\", \"agent_email\", \"agent_phone\"],\n",
    "            (row['policy_number'], row['agent_name'], row['agent_email'], row['agent_phone'])\n",
    "        )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Agents'\")\n",
    "\n",
    "# Cerrar la conexión\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:  ['id', 'first_name', 'last_name', 'email', 'gender', 'policy_number', 'insured_name', 'insured_gender', 'insured_age', 'insured_address', 'insured_city', 'insured_state', 'insured_postal_code', 'insured_country', 'policy_start_date', 'policy_end_date', 'premium_amount', 'deductible_amount', 'coverage_limit', 'policy_type', 'insurance_company', 'agent_name', 'agent_email', 'agent_phone', 'claim_status', 'claim_date', 'claim_amount', 'claim_description', 'payment_status', 'payment_date', 'payment_amount', 'payment_method']\n",
      "Algunas columnas necesarias no existen para la tabla 'Insured'\n"
     ]
    },
    {
     "ename": "ProgrammingError",
     "evalue": "(207, b\"Invalid column name 'nan'.DB-Lib error message 20018, severity 16:\\nGeneral SQL Server error: Check messages from the SQL Server\\n\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMSSQLDatabaseException\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32msrc\\\\pymssql\\\\_pymssql.pyx:447\u001b[0m, in \u001b[0;36mpymssql._pymssql.Cursor.execute\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_mssql.pyx:1125\u001b[0m, in \u001b[0;36mpymssql._mssql.MSSQLConnection.execute_query\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_mssql.pyx:1156\u001b[0m, in \u001b[0;36mpymssql._mssql.MSSQLConnection.execute_query\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_mssql.pyx:1289\u001b[0m, in \u001b[0;36mpymssql._mssql.MSSQLConnection.format_and_run_query\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_mssql.pyx:1852\u001b[0m, in \u001b[0;36mpymssql._mssql.check_cancel_and_raise\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_mssql.pyx:1898\u001b[0m, in \u001b[0;36mpymssql._mssql.raise_MSSQLDatabaseException\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMSSQLDatabaseException\u001b[0m: (207, b\"Invalid column name 'nan'.DB-Lib error message 20018, severity 16:\\nGeneral SQL Server error: Check messages from the SQL Server\\n\")",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 111\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(col \u001b[38;5;129;01min\u001b[39;00m data_cleaned_pd\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_number\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpremium_amount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeductible_amount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoverage_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m data_cleaned_pd\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m--> 111\u001b[0m         \u001b[43minsert_if_not_exists\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPremium\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpolicy_number\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpremium_amount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeductible_amount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoverage_limit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpolicy_number\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpremium_amount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdeductible_amount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcoverage_limit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlgunas columnas necesarias no existen para la tabla \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPremium\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[62], line 78\u001b[0m, in \u001b[0;36minsert_if_not_exists\u001b[1;34m(table, columns, values)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;66;03m# Si no existe, insertar el registro\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     insert_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINSERT INTO \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) VALUES (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(values))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 78\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43minsert_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     conn\u001b[38;5;241m.\u001b[39mcommit()\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_pymssql.pyx:462\u001b[0m, in \u001b[0;36mpymssql._pymssql.Cursor.execute\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mProgrammingError\u001b[0m: (207, b\"Invalid column name 'nan'.DB-Lib error message 20018, severity 16:\\nGeneral SQL Server error: Check messages from the SQL Server\\n\")"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymssql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, regexp_extract, when\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Inicializa la sesión de Spark\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"DataCleaning\").getOrCreate()\n",
    "\n",
    "# Ruta del archivo cargado\n",
    "file_path = \"C:\\\\Users\\\\Usuario\\\\Documents\\\\WILLIAM\\\\DOCUMENTOS\\\\PERSONALES\\\\MONOKERA\\\\MOCK_DATA.csv\"\n",
    "\n",
    "# Leer el archivo CSV y asegurarse de que tiene encabezado y tipo de datos inferido\n",
    "data_3 = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Verificar los nombres de las columnas\n",
    "print(\"Column names: \", data_3.columns)\n",
    "\n",
    "# Limpiar la columna 'id' para extraer solo los números (si están en el formato esperado)\n",
    "data_3 = data_3.withColumn('id', regexp_extract(col('id'), r'\\d+', 0))\n",
    "\n",
    "# Filtrar filas donde 'id' no es nulo y contiene solo números\n",
    "data_3_cleaned = data_3.filter(col('id').rlike('^\\d+$'))\n",
    "\n",
    "# Verificar si las columnas de fecha existen y convertirlas al formato correcto\n",
    "date_columns = [\"claim_date\", \"payment_date\"]\n",
    "for col_name in date_columns:\n",
    "    if col_name in data_3_cleaned.columns:\n",
    "        data_3_cleaned = data_3_cleaned.withColumn(col_name, to_date(col(col_name), \"M/d/yyyy\"))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada\")\n",
    "\n",
    "# Manejo de valores nulos\n",
    "data_cleaned = data_3_cleaned.fillna({\n",
    "    \"insured_age\": 0,\n",
    "    \"insured_gender\": \"Unknown\",\n",
    "    \"claim_status\": \"Unknown\",\n",
    "    \"payment_status\": \"Unknown\",\n",
    "    \"payment_method\": \"Unknown\"\n",
    "})\n",
    "\n",
    "# Convertir columnas numéricas relevantes a tipos adecuados y manejar nulos\n",
    "numeric_columns = [\"claim_amount\", \"payment_amount\"]\n",
    "for col_name in numeric_columns:\n",
    "    if col_name in data_cleaned.columns:\n",
    "        data_cleaned = data_cleaned.withColumn(\n",
    "            col_name, \n",
    "            when(col(col_name).isNull(), None).otherwise(col(col_name)).cast(DoubleType())\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada para conversión a tipo numérico\")\n",
    "\n",
    "# Convertir el DataFrame de Spark a Pandas para usar pymssql\n",
    "data_cleaned_pd = data_cleaned.toPandas()\n",
    "\n",
    "# **CONEXIÓN A SQL SERVER Y ESCRITURA DE DATOS CON pymssql**\n",
    "\n",
    "# Establecer la conexión con SQL Server\n",
    "conn = pymssql.connect(\n",
    "    server=\"LAPTOP-O07NV287\",  # Dirección del servidor\n",
    "    user=\"williamxlr\",         # Nombre de usuario\n",
    "    password=\"Al3xW$1978\",     # Contraseña\n",
    "    database=\"monokera\"        # Nombre de la base de datos\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Función para insertar solo si el registro no existe\n",
    "def insert_if_not_exists(table, columns, values):\n",
    "    # Crear consulta para verificar si el registro ya existe\n",
    "    select_query = f\"SELECT COUNT(*) FROM {table} WHERE {columns[0]} = %s\"\n",
    "    cursor.execute(select_query, (values[0],))\n",
    "    result = cursor.fetchone()\n",
    "    \n",
    "    if result[0] == 0:\n",
    "        # Si no existe, insertar el registro\n",
    "        insert_query = f\"INSERT INTO {table} ({', '.join(columns)}) VALUES ({', '.join(['%s'] * len(values))})\"\n",
    "        cursor.execute(insert_query, values)\n",
    "        conn.commit()\n",
    "    else:\n",
    "        print(f\"Registro con {columns[0]} = {values[0]} ya existe en la tabla {table}.\")\n",
    "\n",
    "# **1. Insertar en la tabla 'Policy'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"policy_number\", \"policy_start_date\", \"policy_end_date\", \"policy_type\", \"insurance_company\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        insert_if_not_exists(\n",
    "            \"Policy\",\n",
    "            [\"policy_number\", \"policy_start_date\", \"policy_end_date\", \"policy_type\", \"insurance_company\"],\n",
    "            (row['policy_number'], row['policy_start_date'], row['policy_end_date'], row['policy_type'], row['insurance_company'])\n",
    "        )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Policy'\")\n",
    "\n",
    "# **2. Insertar en la tabla 'Insured'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"insured_id\", \"policy_number\", \"insured_name\", \"insured_gender\", \"insured_age\",\n",
    "                                                 \"insured_address\", \"insured_city\", \"insured_state\", \"insured_postal_code\", \"insured_country\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        insert_if_not_exists(\n",
    "            \"Insured\",\n",
    "            [\"insured_id\", \"policy_number\", \"insured_name\", \"insured_gender\", \"insured_age\", \n",
    "             \"insured_address\", \"insured_city\", \"insured_state\", \"insured_postal_code\", \"insured_country\"],\n",
    "            (row['insured_id'], row['policy_number'], row['insured_name'], row['insured_gender'], row['insured_age'],\n",
    "             row['insured_address'], row['insured_city'], row['insured_state'], row['insured_postal_code'], row['insured_country'])\n",
    "        )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Insured'\")\n",
    "\n",
    "# **3. Insertar en la tabla 'Premium'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"policy_number\", \"premium_amount\", \"deductible_amount\", \"coverage_limit\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        insert_if_not_exists(\n",
    "            \"Premium\",\n",
    "            [\"policy_number\", \"premium_amount\", \"deductible_amount\", \"coverage_limit\"],\n",
    "            (row['policy_number'], row['premium_amount'], row['deductible_amount'], row['coverage_limit'])\n",
    "        )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Premium'\")\n",
    "\n",
    "# **4. Insertar en la tabla 'Payments'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"policy_number\", \"payment_status\", \"payment_date\", \"payment_amount\", \"payment_method\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        insert_if_not_exists(\n",
    "            \"Payments\",\n",
    "            [\"policy_number\", \"payment_status\", \"payment_date\", \"payment_amount\", \"payment_method\"],\n",
    "            (row['policy_number'], row['payment_status'], row['payment_date'], row['payment_amount'], row['payment_method'])\n",
    "        )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Payments'\")\n",
    "\n",
    "# **5. Insertar en la tabla 'Claims'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"policy_number\", \"claim_status\", \"claim_date\", \"claim_amount\", \"claim_description\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        insert_if_not_exists(\n",
    "            \"Claims\",\n",
    "            [\"policy_number\", \"claim_status\", \"claim_date\", \"claim_amount\", \"claim_description\"],\n",
    "            (row['policy_number'], row['claim_status'], row['claim_date'], row['claim_amount'], row['claim_description'])\n",
    "        )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Claims'\")\n",
    "\n",
    "# **6. Insertar en la tabla 'Agents'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"policy_number\", \"agent_name\", \"agent_email\", \"agent_phone\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        insert_if_not_exists(\n",
    "            \"Agents\",\n",
    "            [\"policy_number\", \"agent_name\", \"agent_email\", \"agent_phone\"],\n",
    "            (row['policy_number'], row['agent_name'], row['agent_email'], row['agent_phone'])\n",
    "        )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Agents'\")\n",
    "\n",
    "# Cerrar la conexión\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id first_name    last_name                      email  gender  \\\n",
      "0  1.0      Bessy  Tomaszynski      btomaszynski0@icio.us    Male   \n",
      "1  2.0       Cher       Gjerde         cgjerde1@hexun.com     NaN   \n",
      "2  3.0   Felicity       Lowman        flowman2@flavors.me    Male   \n",
      "3  4.0        Ive      Jervois          ijervois3@epa.gov    Male   \n",
      "4  5.0     Gretal     Bauckham  gbauckham4@slideshare.net  Female   \n",
      "\n",
      "   policy_number       insured_name insured_gender  insured_age  \\\n",
      "0              1  Bessy Tomaszynski    Desconocido         63.0   \n",
      "1              2        Cher Gjerde         Female         19.0   \n",
      "2              3    Felicity Lowman         Female         43.0   \n",
      "3              4        Ive Jervois           Male         60.0   \n",
      "4              5    Gretal Bauckham         Female         29.0   \n",
      "\n",
      "        insured_address  ...              agent_email        agent_phone  \\\n",
      "0     1 Northland Trail  ...  btomaszynski0@alexa.com  +1 (757) 646-3536   \n",
      "1       98989 Basil Way  ...       cgjerde1@fotki.com  +1 (850) 302-7531   \n",
      "2         2 Derek Place  ...  flowman2@wikispaces.com  +1 (754) 879-3501   \n",
      "3  889 Twin Pines Court  ...     ijervois3@ebay.co.uk                NaN   \n",
      "4  39745 Clarendon Hill  ...        gbauckham4@goo.gl  +1 (480) 648-4238   \n",
      "\n",
      "   claim_status claim_date claim_amount  \\\n",
      "0      Approved 2022-12-11      7589.60   \n",
      "1        Denied 2022-08-20      5777.00   \n",
      "2       Pending 2022-05-04      1684.43   \n",
      "3      Approved 2022-08-09      8160.13   \n",
      "4      Approved 2022-06-08      9694.67   \n",
      "\n",
      "                                   claim_description  payment_status  \\\n",
      "0  Duis bibendum, felis sed interdum venenatis, t...         Overdue   \n",
      "1  Quisque id justo sit amet sapien dignissim ves...         Overdue   \n",
      "2  In sagittis dui vel nisl. Duis ac nibh. Fusce ...         Pending   \n",
      "3  In hac habitasse platea dictumst. Etiam faucib...         Pending   \n",
      "4  Praesent id massa id nisl venenatis lacinia. A...         Pending   \n",
      "\n",
      "   payment_date  payment_amount payment_method  \n",
      "0    2022-03-19          644.58    Credit Card  \n",
      "1    2022-06-03          912.79          Check  \n",
      "2    2022-08-23          995.75          Check  \n",
      "3    2022-11-18          190.06  Bank Transfer  \n",
      "4    2022-11-26          645.52          Check  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta del archivo cargado\n",
    "file_path = \"C:\\\\Users\\\\Usuario\\\\Documents\\\\WILLIAM\\\\DOCUMENTOS\\\\PERSONALES\\\\MONOKERA\\\\MOCK_DATA.csv\"\n",
    "\n",
    "# Cargar el archivo para inspeccionar su contenido\n",
    "data_2 = pd.read_csv(file_path)\n",
    "\n",
    "# Convertir fechas al formato correcto\n",
    "date_columns = [\"claim_date\", \"payment_date\"]\n",
    "for col_name in date_columns:\n",
    "    data_2[col_name] = pd.to_datetime(data_2[col_name], format=\"%m/%d/%Y\", errors=\"coerce\")\n",
    "\n",
    "# Manejo de valores nulos\n",
    "data_2.fillna({\n",
    "    \"insured_age\": 0,\n",
    "    \"insured_gender\": \"Desconocido\",\n",
    "    \"claim_status\": \"Unknown\",\n",
    "    \"payment_status\": \"Unknown\",\n",
    "    \"payment_method\": \"Unknown\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Convertir columnas numéricas relevantes a tipos adecuados\n",
    "numeric_columns = [\"claim_amount\", \"payment_amount\"]\n",
    "for col_name in numeric_columns:\n",
    "    data_2[col_name] = pd.to_numeric(data_2[col_name], errors=\"coerce\")\n",
    "\n",
    "# Mostrar las primeras filas después de limpieza\n",
    "print(data_2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:  ['id', 'first_name', 'last_name', 'email', 'gender', 'policy_number', 'insured_name', 'insured_gender', 'insured_age', 'insured_address', 'insured_city', 'insured_state', 'insured_postal_code', 'insured_country', 'policy_start_date', 'policy_end_date', 'premium_amount', 'deductible_amount', 'coverage_limit', 'policy_type', 'insurance_company', 'agent_name', 'agent_email', 'agent_phone', 'claim_status', 'claim_date', 'claim_amount', 'claim_description', 'payment_status', 'payment_date', 'payment_amount', 'payment_method']\n",
      "Algunas columnas necesarias no existen para la tabla 'Policy'\n",
      "Algunas columnas necesarias no existen para la tabla 'Insured'\n",
      "Algunas columnas necesarias no existen para la tabla 'Premium'\n"
     ]
    },
    {
     "ename": "ProgrammingError",
     "evalue": "(207, b\"Invalid column name 'policy_id'.DB-Lib error message 20018, severity 16:\\nGeneral SQL Server error: Check messages from the SQL Server\\n\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMSSQLDatabaseException\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32msrc\\\\pymssql\\\\_pymssql.pyx:447\u001b[0m, in \u001b[0;36mpymssql._pymssql.Cursor.execute\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_mssql.pyx:1125\u001b[0m, in \u001b[0;36mpymssql._mssql.MSSQLConnection.execute_query\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_mssql.pyx:1156\u001b[0m, in \u001b[0;36mpymssql._mssql.MSSQLConnection.execute_query\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_mssql.pyx:1289\u001b[0m, in \u001b[0;36mpymssql._mssql.MSSQLConnection.format_and_run_query\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_mssql.pyx:1852\u001b[0m, in \u001b[0;36mpymssql._mssql.check_cancel_and_raise\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_mssql.pyx:1898\u001b[0m, in \u001b[0;36mpymssql._mssql.raise_MSSQLDatabaseException\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMSSQLDatabaseException\u001b[0m: (207, b\"Invalid column name 'policy_id'.DB-Lib error message 20018, severity 16:\\nGeneral SQL Server error: Check messages from the SQL Server\\n\")",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 119\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(col \u001b[38;5;129;01min\u001b[39;00m data_cleaned_pd\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_number\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpayment_status\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpayment_date\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpayment_amount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpayment_method\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m data_cleaned_pd\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m--> 119\u001b[0m         \u001b[43minsert_if_not_exists\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPayments\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpolicy_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpayment_status\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpayment_date\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpayment_amount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpayment_method\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpolicy_number\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpayment_status\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpayment_date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpayment_amount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpayment_method\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlgunas columnas necesarias no existen para la tabla \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPayments\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[61], line 72\u001b[0m, in \u001b[0;36minsert_if_not_exists\u001b[1;34m(table, columns, values)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minsert_if_not_exists\u001b[39m(table, columns, values):\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# Crear consulta para verificar si el registro ya existe\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     select_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT COUNT(*) FROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m WHERE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m = %s\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 72\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselect_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     result \u001b[38;5;241m=\u001b[39m cursor\u001b[38;5;241m.\u001b[39mfetchone()\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;66;03m# Si no existe, insertar el registro\u001b[39;00m\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_pymssql.pyx:462\u001b[0m, in \u001b[0;36mpymssql._pymssql.Cursor.execute\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mProgrammingError\u001b[0m: (207, b\"Invalid column name 'policy_id'.DB-Lib error message 20018, severity 16:\\nGeneral SQL Server error: Check messages from the SQL Server\\n\")"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymssql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, regexp_extract, when\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Inicializa la sesión de Spark\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"DataCleaning\").getOrCreate()\n",
    "\n",
    "# Ruta del archivo cargado\n",
    "file_path = \"C:\\\\Users\\\\Usuario\\\\Documents\\\\WILLIAM\\\\DOCUMENTOS\\\\PERSONALES\\\\MONOKERA\\\\MOCK_DATA.csv\"\n",
    "\n",
    "# Leer el archivo CSV y asegurarse de que tiene encabezado y tipo de datos inferido\n",
    "data_3 = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Verificar los nombres de las columnas\n",
    "print(\"Column names: \", data_3.columns)\n",
    "\n",
    "# Limpiar la columna 'id' para extraer solo los números (si están en el formato esperado)\n",
    "data_3 = data_3.withColumn('id', regexp_extract(col('id'), r'\\d+', 0))\n",
    "\n",
    "# Filtrar filas donde 'id' no es nulo y contiene solo números\n",
    "data_3_cleaned = data_3.filter(col('id').rlike('^\\d+$'))\n",
    "\n",
    "# Verificar si las columnas de fecha existen y convertirlas al formato correcto\n",
    "date_columns = [\"claim_date\", \"payment_date\"]\n",
    "for col_name in date_columns:\n",
    "    if col_name in data_3_cleaned.columns:\n",
    "        data_3_cleaned = data_3_cleaned.withColumn(col_name, to_date(col(col_name), \"M/d/yyyy\"))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada\")\n",
    "\n",
    "# Manejo de valores nulos\n",
    "data_cleaned = data_3_cleaned.fillna({\n",
    "    \"insured_age\": 0,\n",
    "    \"insured_gender\": \"Unknown\",\n",
    "    \"claim_status\": \"Unknown\",\n",
    "    \"payment_status\": \"Unknown\",\n",
    "    \"payment_method\": \"Unknown\"\n",
    "})\n",
    "\n",
    "# Convertir columnas numéricas relevantes a tipos adecuados y manejar nulos\n",
    "numeric_columns = [\"claim_amount\", \"payment_amount\"]\n",
    "for col_name in numeric_columns:\n",
    "    if col_name in data_cleaned.columns:\n",
    "        data_cleaned = data_cleaned.withColumn(\n",
    "            col_name, \n",
    "            when(col(col_name).isNull(), None).otherwise(col(col_name)).cast(DoubleType())\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada para conversión a tipo numérico\")\n",
    "\n",
    "# Convertir el DataFrame de Spark a Pandas para usar pymssql\n",
    "data_cleaned_pd = data_cleaned.toPandas()\n",
    "\n",
    "# **CONEXIÓN A SQL SERVER Y ESCRITURA DE DATOS CON pymssql**\n",
    "\n",
    "# Establecer la conexión con SQL Server\n",
    "conn = pymssql.connect(\n",
    "    server=\"LAPTOP-O07NV287\",  # Dirección del servidor\n",
    "    user=\"williamxlr\",         # Nombre de usuario\n",
    "    password=\"Al3xW$1978\",     # Contraseña\n",
    "    database=\"monokera\"        # Nombre de la base de datos\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Función para insertar solo si el registro no existe\n",
    "def insert_if_not_exists(table, columns, values):\n",
    "    # Crear consulta para verificar si el registro ya existe\n",
    "    select_query = f\"SELECT COUNT(*) FROM {table} WHERE {columns[0]} = %s\"\n",
    "    cursor.execute(select_query, (values[0],))\n",
    "    result = cursor.fetchone()\n",
    "    \n",
    "    if result[0] == 0:\n",
    "        # Si no existe, insertar el registro\n",
    "        insert_query = f\"INSERT INTO {table} ({', '.join(columns)}) VALUES ({', '.join(['%s'] * len(values))})\"\n",
    "        cursor.execute(insert_query, values)\n",
    "        conn.commit()\n",
    "    else:\n",
    "        print(f\"Registro con {columns[0]} = {values[0]} ya existe en la tabla {table}.\")\n",
    "\n",
    "# **1. Insertar en la tabla 'Policy'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"policy_number\", \"start_date\", \"end_date\", \"insured_id\", \"agent_id\", \"premium_id\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        insert_if_not_exists(\n",
    "            \"Policy\",\n",
    "            [\"policy_number\", \"start_date\", \"end_date\", \"insured_id\", \"agent_id\", \"premium_id\"],\n",
    "            (row['policy_number'], row['start_date'], row['end_date'], row['insured_id'], row['agent_id'], row['premium_id'])\n",
    "        )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Policy'\")\n",
    "\n",
    "# **2. Insertar en la tabla 'Insured'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"insured_id\", \"first_name\", \"last_name\", \"gender\", \"dob\", \"address\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        insert_if_not_exists(\n",
    "            \"Insured\",\n",
    "            [\"insured_id\", \"first_name\", \"last_name\", \"gender\", \"dob\", \"address\"],\n",
    "            (row['insured_id'], row['first_name'], row['last_name'], row['gender'], row['dob'], row['address'])\n",
    "        )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Insured'\")\n",
    "\n",
    "# **3. Insertar en la tabla 'Premium'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"premium_id\", \"premium_amount\", \"deductible_amount\", \"coverage_limit\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        insert_if_not_exists(\n",
    "            \"Premium\",\n",
    "            [\"premium_id\", \"premium_amount\", \"deductible_amount\", \"coverage_limit\"],\n",
    "            (row['premium_id'], row['premium_amount'], row['deductible_amount'], row['coverage_limit'])\n",
    "        )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Premium'\")\n",
    "\n",
    "# **4. Insertar en la tabla 'Payments'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"policy_number\", \"payment_status\", \"payment_date\", \"payment_amount\", \"payment_method\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        insert_if_not_exists(\n",
    "            \"Payments\",\n",
    "            [\"policy_id\", \"payment_status\", \"payment_date\", \"payment_amount\", \"payment_method\"],\n",
    "            (row['policy_number'], row['payment_status'], row['payment_date'], row['payment_amount'], row['payment_method'])\n",
    "        )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Payments'\")\n",
    "\n",
    "# **5. Insertar en la tabla 'Claims'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"policy_number\", \"claim_status\", \"claim_date\", \"claim_amount\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        insert_if_not_exists(\n",
    "            \"Claims\",\n",
    "            [\"policy_id\", \"claim_status\", \"claim_date\", \"claim_amount\"],\n",
    "            (row['policy_number'], row['claim_status'], row['claim_date'], row['claim_amount'])\n",
    "        )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Claims'\")\n",
    "\n",
    "# **6. Insertar en la tabla 'Agents'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"agent_id\", \"agent_name\", \"agent_channel\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        insert_if_not_exists(\n",
    "            \"Agents\",\n",
    "            [\"agent_id\", \"agent_name\", \"agent_channel\"],\n",
    "            (row['agent_id'], row['agent_name'], row['agent_channel'])\n",
    "        )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Agents'\")\n",
    "\n",
    "# Cerrar la conexión\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:  ['id', 'first_name', 'last_name', 'email', 'gender', 'policy_number', 'insured_name', 'insured_gender', 'insured_age', 'insured_address', 'insured_city', 'insured_state', 'insured_postal_code', 'insured_country', 'policy_start_date', 'policy_end_date', 'premium_amount', 'deductible_amount', 'coverage_limit', 'policy_type', 'insurance_company', 'agent_name', 'agent_email', 'agent_phone', 'claim_status', 'claim_date', 'claim_amount', 'claim_description', 'payment_status', 'payment_date', 'payment_amount', 'payment_method']\n",
      "Algunas columnas necesarias no existen para la tabla 'Policy'\n",
      "Algunas columnas necesarias no existen para la tabla 'Insured'\n",
      "Algunas columnas necesarias no existen para la tabla 'Premium'\n",
      "Algunas columnas necesarias no existen para la tabla 'Agents'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymssql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, regexp_extract, when\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Inicializa la sesión de Spark\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"DataCleaning\").getOrCreate()\n",
    "\n",
    "# Ruta del archivo cargado\n",
    "file_path = \"C:\\\\Users\\\\Usuario\\\\Documents\\\\WILLIAM\\\\DOCUMENTOS\\\\PERSONALES\\\\MONOKERA\\\\MOCK_DATA.csv\"\n",
    "\n",
    "# Leer el archivo CSV y asegurarse de que tiene encabezado y tipo de datos inferido\n",
    "data_3 = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Verificar los nombres de las columnas\n",
    "print(\"Column names: \", data_3.columns)\n",
    "\n",
    "# Limpiar la columna 'id' para extraer solo los números (si están en el formato esperado)\n",
    "data_3 = data_3.withColumn('id', regexp_extract(col('id'), r'\\d+', 0))\n",
    "\n",
    "# Filtrar filas donde 'id' no es nulo y contiene solo números\n",
    "data_3_cleaned = data_3.filter(col('id').rlike('^\\d+$'))\n",
    "\n",
    "# Verificar si las columnas de fecha existen y convertirlas al formato correcto\n",
    "date_columns = [\"claim_date\", \"payment_date\"]\n",
    "for col_name in date_columns:\n",
    "    if col_name in data_3_cleaned.columns:\n",
    "        data_3_cleaned = data_3_cleaned.withColumn(col_name, to_date(col(col_name), \"M/d/yyyy\"))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada\")\n",
    "\n",
    "# Manejo de valores nulos\n",
    "data_cleaned = data_3_cleaned.fillna({\n",
    "    \"insured_age\": 0,\n",
    "    \"insured_gender\": \"Unknown\",\n",
    "    \"claim_status\": \"Unknown\",\n",
    "    \"payment_status\": \"Unknown\",\n",
    "    \"payment_method\": \"Unknown\"\n",
    "})\n",
    "\n",
    "# Convertir columnas numéricas relevantes a tipos adecuados y manejar nulos\n",
    "numeric_columns = [\"claim_amount\", \"payment_amount\"]\n",
    "for col_name in numeric_columns:\n",
    "    if col_name in data_cleaned.columns:\n",
    "        data_cleaned = data_cleaned.withColumn(\n",
    "            col_name, \n",
    "            when(col(col_name).isNull(), None).otherwise(col(col_name)).cast(DoubleType())\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada para conversión a tipo numérico\")\n",
    "\n",
    "# Convertir el DataFrame de Spark a Pandas para usar pymssql\n",
    "data_cleaned_pd = data_cleaned.toPandas()\n",
    "\n",
    "# **CONEXIÓN A SQL SERVER Y ESCRITURA DE DATOS CON pymssql**\n",
    "\n",
    "# Establecer la conexión con SQL Server\n",
    "conn = pymssql.connect(\n",
    "    server=\"LAPTOP-O07NV287\",  # Dirección del servidor\n",
    "    user=\"williamxlr\",         # Nombre de usuario\n",
    "    password=\"Al3xW$1978\",     # Contraseña\n",
    "    database=\"monokera\"        # Nombre de la base de datos\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Función para insertar solo si el registro no existe\n",
    "def insert_if_not_exists(table, columns, values):\n",
    "    # Crear consulta para verificar si el registro ya existe\n",
    "    select_query = f\"SELECT COUNT(*) FROM {table} WHERE {columns[0]} = %s\"\n",
    "    cursor.execute(select_query, (values[0],))\n",
    "    result = cursor.fetchone()\n",
    "    \n",
    "    if result[0] == 0:\n",
    "        # Si no existe, insertar el registro\n",
    "        insert_query = f\"INSERT INTO {table} ({', '.join(columns)}) VALUES ({', '.join(['%s'] * len(values))})\"\n",
    "        cursor.execute(insert_query, values)\n",
    "        conn.commit()\n",
    "    else:\n",
    "        print(f\"Registro con {columns[0]} = {values[0]} ya existe en la tabla {table}.\")\n",
    "\n",
    "# **1. Insertar en la tabla 'Policy'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"policy_number\", \"policy_start_date\", \"policy_end_date\", \"insured_id\", \"agent_id\", \"premium_id\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        insert_if_not_exists(\n",
    "            \"Policy\",\n",
    "            [\"policy_number\", \"policy_start_date\", \"policy_end_date\", \"insured_id\", \"agent_id\", \"premium_id\"],\n",
    "            (row['policy_number'], row['policy_start_date'], row['policy_end_date'], row['insured_id'], row['agent_id'], row['premium_id'])\n",
    "        )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Policy'\")\n",
    "\n",
    "# **2. Insertar en la tabla 'Insured'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"insured_id\", \"first_name\", \"last_name\", \"insured_gender\", \"insured_age\", \"insured_address\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        insert_if_not_exists(\n",
    "            \"Insured\",\n",
    "            [\"insured_id\", \"first_name\", \"last_name\", \"insured_gender\", \"insured_age\", \"insured_address\"],\n",
    "            (row['insured_id'], row['first_name'], row['last_name'], row['insured_gender'], row['insured_age'], row['insured_address'])\n",
    "        )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Insured'\")\n",
    "\n",
    "# **3. Insertar en la tabla 'Premium'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"premium_id\", \"premium_amount\", \"deductible_amount\", \"coverage_limit\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        insert_if_not_exists(\n",
    "            \"Premium\",\n",
    "            [\"premium_id\", \"premium_amount\", \"deductible_amount\", \"coverage_limit\"],\n",
    "            (row['premium_id'], row['premium_amount'], row['deductible_amount'], row['coverage_limit'])\n",
    "        )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Premium'\")\n",
    "\n",
    "# **4. Insertar en la tabla 'Payments'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"policy_number\", \"payment_status\", \"payment_date\", \"payment_amount\", \"payment_method\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        # Obtener policy_number desde Policy usando el policy_number\n",
    "        cursor.execute(\"SELECT policy_number FROM Policy WHERE policy_number = %s\", (row['policy_number'],))\n",
    "        policy_number = cursor.fetchone()\n",
    "        if policy_number:\n",
    "            insert_if_not_exists(\n",
    "                \"Payments\",\n",
    "                [\"policy_number\", \"payment_status\", \"payment_date\", \"payment_amount\", \"payment_method\"],\n",
    "                (policy_number[0], row['payment_status'], row['payment_date'], row['payment_amount'], row['payment_method'])\n",
    "            )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Payments'\")\n",
    "\n",
    "# **5. Insertar en la tabla 'Claims'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"policy_number\", \"claim_status\", \"claim_date\", \"claim_amount\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        # Obtener policy_number desde policy_number usando el policy_number\n",
    "        cursor.execute(\"SELECT policy_number FROM Policy WHERE policy_number = %s\", (row['policy_number'],))\n",
    "        policy_number = cursor.fetchone()\n",
    "        if policy_number:\n",
    "            insert_if_not_exists(\n",
    "                \"Claims\",\n",
    "                [\"policy_number\", \"claim_status\", \"claim_date\", \"claim_amount\"],\n",
    "                (policy_number[0], row['claim_status'], row['claim_date'], row['claim_amount'])\n",
    "            )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Claims'\")\n",
    "\n",
    "# **6. Insertar en la tabla 'Agents'**\n",
    "if all(col in data_cleaned_pd.columns for col in [\"agent_id\", \"agent_name\", \"agent_email\"]):\n",
    "    for _, row in data_cleaned_pd.iterrows():\n",
    "        insert_if_not_exists(\n",
    "            \"Agents\",\n",
    "            [\"agent_id\", \"agent_name\", \"agent_email\"],\n",
    "            (row['agent_id'], row['agent_name'], row['agent_email'])\n",
    "        )\n",
    "else:\n",
    "    print(\"Algunas columnas necesarias no existen para la tabla 'Agents'\")\n",
    "\n",
    "# Cerrar la conexión\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
