{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:  ['id', 'first_name', 'last_name', 'email', 'gender', 'policy_number', 'insured_name', 'insured_gender', 'insured_age', 'insured_address', 'insured_city', 'insured_state', 'insured_postal_code', 'insured_country', 'policy_start_date', 'policy_end_date', 'premium_amount', 'deductible_amount', 'coverage_limit', 'policy_type', 'insurance_company', 'agent_name', 'agent_email', 'agent_phone', 'claim_status', 'claim_date', 'claim_amount', 'claim_description', 'payment_status', 'payment_date', 'payment_amount', 'payment_method']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, regexp_extract\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import functions as F  # Importar funciones como F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType\n",
    "\n",
    "# Inicializa la sesión de Spark\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"DataCleaning\").getOrCreate()\n",
    "\n",
    "# Ruta del archivo cargado\n",
    "file_path = \"C:\\\\Users\\\\Usuario\\\\Documents\\\\WILLIAM\\\\DOCUMENTOS\\\\PERSONALES\\\\MONOKERA\\\\MOCK_DATA.csv\"\n",
    "\n",
    "# Leer el archivo CSV y asegurarse de que tiene encabezado y tipo de datos inferido\n",
    "data_3 = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Verificar los nombres de las columnas\n",
    "print(\"Column names: \", data_3.columns)\n",
    "\n",
    "# Limpiar la columna 'id' para extraer solo los números (si están en el formato esperado)\n",
    "# Usamos expresiones regulares para extraer solo la parte numérica\n",
    "data_3 = data_3.withColumn('id', regexp_extract(col('id'), r'\\d+', 0))\n",
    "\n",
    "# Filtrar filas donde 'id' no es nulo y contiene solo números\n",
    "data_3_cleaned = data_3.filter(col('id').rlike('^\\d+$'))\n",
    "\n",
    "# Verificar si las columnas de fecha existen y convertirlas al formato correcto\n",
    "date_columns = [\"claim_date\", \"payment_date\"]\n",
    "for col_name in date_columns:\n",
    "    if col_name in data_3_cleaned.columns:\n",
    "        data_3_cleaned = data_3_cleaned.withColumn(col_name, to_date(col(col_name), \"M/d/yyyy\"))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada\")\n",
    "\n",
    "# Manejo de valores nulos\n",
    "# Rellenar valores nulos con valores predeterminados\n",
    "data_cleaned = data_3_cleaned.fillna({\n",
    "    \"insured_age\": 0,\n",
    "    \"insured_gender\": \"Unknown\",\n",
    "    \"claim_status\": \"Unknown\",\n",
    "    \"payment_status\": \"Unknown\",\n",
    "    \"payment_method\": \"Unknown\"\n",
    "})\n",
    "\n",
    "# Convertir columnas numéricas relevantes a tipos adecuados\n",
    "numeric_columns = [\"claim_amount\", \"payment_amount\"]\n",
    "for col_name in numeric_columns:\n",
    "    if col_name in data_cleaned.columns:\n",
    "        data_cleaned = data_cleaned.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada para conversión a tipo numérico\")\n",
    "\n",
    "# Mostrar las primeras filas después de limpieza\n",
    "#data_cleaned.show(5)\n",
    "\n",
    "# Filtrar y seleccionar las columnas necesarias para 'data_claim'\n",
    "data_claim = data_cleaned.filter(F.col('claim_date').isNotNull()) \\\n",
    "    .select(\n",
    "        F.col('policy_number').alias('id'),\n",
    "        'claim_status',\n",
    "        'claim_date',\n",
    "        'claim_amount',\n",
    "        'claim_description'\n",
    "    )\n",
    "\n",
    "#data_claim.show(5)\n",
    "# Contar el número de pólizas que tienen reclamaciones\n",
    "#reclamaciones_count = data_cleaned.select('policy_number').distinct().count()\n",
    "\n",
    "#print(f\"Número de pólizas con reclamaciones: {reclamaciones_count}\")\n",
    "\n",
    "\n",
    "data_insured = data_cleaned.groupBy(\"policy_number\").agg(\n",
    "    F.first('insured_name').alias('insured_name'),\n",
    "    F.first('insured_gender').alias('insured_gender'),\n",
    "    F.first('insured_age').alias('insured_age'),\n",
    "    F.first('insured_address').alias('insured_address'),\n",
    "    F.first('insured_city').alias('insured_city'),\n",
    "    F.first('insured_state').alias('insured_state'),\n",
    "    F.first('insured_postal_code').alias('insured_postal_code'),\n",
    "    F.first('insured_country').alias('insured_country')\n",
    ").withColumnRenamed(\"policy_number\", \"id\")\n",
    "\n",
    "#data_insured.show(5)\n",
    "\n",
    "\n",
    "# Filtrar las columnas que contienen \"payment\" en su nombre\n",
    "payment_columns = [col_name for col_name in data_cleaned.columns if \"payment\" in col_name]\n",
    "\n",
    "# Crear el DataFrame con las columnas de payment y policy_number como id\n",
    "data_payments = data_cleaned.select(\n",
    "    F.col('policy_number').alias('id'),\n",
    "    *payment_columns\n",
    ")\n",
    "\n",
    "# Filtrar filas donde payment_status sea diferente de 'Unknown'\n",
    "data_payments_filtered = data_payments.filter(F.col('payment_status') != 'Unknown')\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame filtrado\n",
    "#data_payments_filtered.show()\n",
    "\n",
    "\n",
    "data_premium = data_cleaned.groupBy(\"policy_number\").agg(\n",
    "    F.first('premium_amount').alias('premium_amount'),\n",
    "    F.first('deductible_amount').alias('deductible_amount'),\n",
    "    F.first('coverage_limit').alias('coverage_limit')  \n",
    ").withColumnRenamed(\"policy_number\", \"id\")\n",
    "\n",
    "#data_premium.show(5)\n",
    "\n",
    "# Agrupar por 'policy_number' y obtener los primeros valores de 'agent_name', 'agent_email' y 'agent_phone'\n",
    "data_agents = data_cleaned.groupBy(\"policy_number\").agg(\n",
    "    F.first('agent_name').alias('agent_name'),\n",
    "    F.first('agent_email').alias('agent_email'),\n",
    "    F.first('agent_phone').alias('agent_phone')\n",
    ").withColumnRenamed(\"policy_number\", \"id\")\n",
    "\n",
    "# Filtrar para eliminar agentes con 'agent_name' nulo o 'Unknown'\n",
    "data_agents_filtered = data_agents.filter(\n",
    "    (F.col('agent_name').isNotNull()) & (F.col('agent_name') != 'Unknown')\n",
    ")\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "#data_agents_filtered.show(5)\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "data_cleaned = data_cleaned.withColumn(\n",
    "    'policy_start_date', to_date(col('policy_start_date'), 'yyyy-MM-dd')\n",
    ").withColumn(\n",
    "    'policy_end_date', to_date(col('policy_end_date'), 'yyyy-MM-dd')\n",
    ")\n",
    "# Asegurarte de que las columnas necesarias existen en data_cleaned\n",
    "required_columns = ['policy_number', 'policy_start_date', 'policy_end_date', 'policy_type', 'insurance_company']\n",
    "\n",
    "# Filtrar solo las columnas necesarias y crear el DataFrame 'data_policy'\n",
    "data_policy = data_cleaned.select(\n",
    "    col('policy_number'),\n",
    "    col('policy_start_date'),\n",
    "    col('policy_end_date'),\n",
    "    col('policy_type'),\n",
    "    col('insurance_company')\n",
    ")\n",
    "\n",
    "# Mostrar las primeras filas del nuevo DataFrame\n",
    "#data_policy.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:  ['id', 'first_name', 'last_name', 'email', 'gender', 'policy_number', 'insured_name', 'insured_gender', 'insured_age', 'insured_address', 'insured_city', 'insured_state', 'insured_postal_code', 'insured_country', 'policy_start_date', 'policy_end_date', 'premium_amount', 'deductible_amount', 'coverage_limit', 'policy_type', 'insurance_company', 'agent_name', 'agent_email', 'agent_phone', 'claim_status', 'claim_date', 'claim_amount', 'claim_description', 'payment_status', 'payment_date', 'payment_amount', 'payment_method']\n"
     ]
    },
    {
     "ename": "IntegrityError",
     "evalue": "(515, b\"Cannot insert the value NULL into column 'policy_end_date', table 'monokera.dbo.Policy'; column does not allow nulls. INSERT fails.DB-Lib error message 20018, severity 16:\\nGeneral SQL Server error: Check messages from the SQL Server\\n\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMSSQLDatabaseException\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32msrc\\\\pymssql\\\\_pymssql.pyx:447\u001b[0m, in \u001b[0;36mpymssql._pymssql.Cursor.execute\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_mssql.pyx:1125\u001b[0m, in \u001b[0;36mpymssql._mssql.MSSQLConnection.execute_query\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_mssql.pyx:1156\u001b[0m, in \u001b[0;36mpymssql._mssql.MSSQLConnection.execute_query\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_mssql.pyx:1289\u001b[0m, in \u001b[0;36mpymssql._mssql.MSSQLConnection.format_and_run_query\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_mssql.pyx:1852\u001b[0m, in \u001b[0;36mpymssql._mssql.check_cancel_and_raise\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_mssql.pyx:1898\u001b[0m, in \u001b[0;36mpymssql._mssql.raise_MSSQLDatabaseException\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMSSQLDatabaseException\u001b[0m: (515, b\"Cannot insert the value NULL into column 'policy_end_date', table 'monokera.dbo.Policy'; column does not allow nulls. INSERT fails.DB-Lib error message 20018, severity 16:\\nGeneral SQL Server error: Check messages from the SQL Server\\n\")",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mIntegrityError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 142\u001b[0m\n\u001b[0;32m    139\u001b[0m     conn\u001b[38;5;241m.\u001b[39mcommit()\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Cargar DataFrames a las tablas de SQL Server (asegurándote de que las tablas ya están creadas)\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m \u001b[43mload_to_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpolicy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m load_to_sql(data_agents_filtered, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124magents\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    144\u001b[0m load_to_sql(data_premium, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpremium\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[75], line 138\u001b[0m, in \u001b[0;36mload_to_sql\u001b[1;34m(df, table_name)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcollect():\n\u001b[0;32m    137\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINSERT INTO \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(df\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) VALUES (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 138\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m conn\u001b[38;5;241m.\u001b[39mcommit()\n",
      "File \u001b[1;32msrc\\\\pymssql\\\\_pymssql.pyx:464\u001b[0m, in \u001b[0;36mpymssql._pymssql.Cursor.execute\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIntegrityError\u001b[0m: (515, b\"Cannot insert the value NULL into column 'policy_end_date', table 'monokera.dbo.Policy'; column does not allow nulls. INSERT fails.DB-Lib error message 20018, severity 16:\\nGeneral SQL Server error: Check messages from the SQL Server\\n\")"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, regexp_extract\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import functions as F  # Importar funciones como F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType\n",
    "import pymssql\n",
    "\n",
    "# Inicializa la sesión de Spark\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"DataCleaning\").getOrCreate()\n",
    "\n",
    "# Ruta del archivo cargado\n",
    "file_path = \"C:\\\\Users\\\\Usuario\\\\Documents\\\\WILLIAM\\\\DOCUMENTOS\\\\PERSONALES\\\\MONOKERA\\\\MOCK_DATA.csv\"\n",
    "\n",
    "# Leer el archivo CSV y asegurarse de que tiene encabezado y tipo de datos inferido\n",
    "data_3 = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Verificar los nombres de las columnas\n",
    "print(\"Column names: \", data_3.columns)\n",
    "\n",
    "# Limpiar la columna 'id' para extraer solo los números (si están en el formato esperado)\n",
    "# Usamos expresiones regulares para extraer solo la parte numérica\n",
    "data_3 = data_3.withColumn('id', regexp_extract(col('id'), r'\\d+', 0))\n",
    "\n",
    "# Filtrar filas donde 'id' no es nulo y contiene solo números\n",
    "data_3_cleaned = data_3.filter(col('id').rlike('^\\d+$'))\n",
    "\n",
    "# Verificar si las columnas de fecha existen y convertirlas al formato correcto\n",
    "date_columns = [\"claim_date\", \"payment_date\"]\n",
    "for col_name in date_columns:\n",
    "    if col_name in data_3_cleaned.columns:\n",
    "        data_3_cleaned = data_3_cleaned.withColumn(col_name, to_date(col(col_name), \"M/d/yyyy\"))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada\")\n",
    "\n",
    "# Manejo de valores nulos\n",
    "# Rellenar valores nulos con valores predeterminados\n",
    "data_cleaned = data_3_cleaned.fillna({\n",
    "    \"insured_age\": 0,\n",
    "    \"insured_gender\": \"Unknown\",\n",
    "    \"claim_status\": \"Unknown\",\n",
    "    \"payment_status\": \"Unknown\",\n",
    "    \"payment_method\": \"Unknown\"\n",
    "})\n",
    "\n",
    "# Convertir columnas numéricas relevantes a tipos adecuados\n",
    "numeric_columns = [\"claim_amount\", \"payment_amount\"]\n",
    "for col_name in numeric_columns:\n",
    "    if col_name in data_cleaned.columns:\n",
    "        data_cleaned = data_cleaned.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada para conversión a tipo numérico\")\n",
    "\n",
    "# Filtrar y seleccionar las columnas necesarias para 'data_claim'\n",
    "data_claim = data_cleaned.filter(F.col('claim_date').isNotNull()) \\\n",
    "    .select(\n",
    "        F.col('policy_number').alias('id'),\n",
    "        'claim_status',\n",
    "        'claim_date',\n",
    "        'claim_amount',\n",
    "        'claim_description'\n",
    "    )\n",
    "\n",
    "# Agrupar por 'policy_number' y obtener los primeros valores de 'insured_name', 'insured_gender', 'insured_age', etc.\n",
    "data_insured = data_cleaned.groupBy(\"policy_number\").agg(\n",
    "    F.first('insured_name').alias('insured_name'),\n",
    "    F.first('insured_gender').alias('insured_gender'),\n",
    "    F.first('insured_age').alias('insured_age'),\n",
    "    F.first('insured_address').alias('insured_address'),\n",
    "    F.first('insured_city').alias('insured_city'),\n",
    "    F.first('insured_state').alias('insured_state'),\n",
    "    F.first('insured_postal_code').alias('insured_postal_code'),\n",
    "    F.first('insured_country').alias('insured_country')\n",
    ").withColumnRenamed(\"policy_number\", \"id\")\n",
    "\n",
    "# Filtrar las columnas que contienen \"payment\" en su nombre\n",
    "payment_columns = [col_name for col_name in data_cleaned.columns if \"payment\" in col_name]\n",
    "\n",
    "# Crear el DataFrame con las columnas de payment y policy_number como id\n",
    "data_payments = data_cleaned.select(\n",
    "    F.col('policy_number').alias('id'),\n",
    "    *payment_columns\n",
    ")\n",
    "\n",
    "# Filtrar filas donde payment_status sea diferente de 'Unknown'\n",
    "data_payments_filtered = data_payments.filter(F.col('payment_status') != 'Unknown')\n",
    "\n",
    "# Agrupar por 'policy_number' y obtener los primeros valores de 'premium_amount', 'deductible_amount', etc.\n",
    "data_premium = data_cleaned.groupBy(\"policy_number\").agg(\n",
    "    F.first('premium_amount').alias('premium_amount'),\n",
    "    F.first('deductible_amount').alias('deductible_amount'),\n",
    "    F.first('coverage_limit').alias('coverage_limit')  \n",
    ").withColumnRenamed(\"policy_number\", \"id\")\n",
    "\n",
    "# Agrupar por 'policy_number' y obtener los primeros valores de 'agent_name', 'agent_email', 'agent_phone'\n",
    "data_agents = data_cleaned.groupBy(\"policy_number\").agg(\n",
    "    F.first('agent_name').alias('agent_name'),\n",
    "    F.first('agent_email').alias('agent_email'),\n",
    "    F.first('agent_phone').alias('agent_phone')\n",
    ").withColumnRenamed(\"policy_number\", \"id\")\n",
    "\n",
    "# Filtrar para eliminar agentes con 'agent_name' nulo o 'Unknown'\n",
    "data_agents_filtered = data_agents.filter(\n",
    "    (F.col('agent_name').isNotNull()) & (F.col('agent_name') != 'Unknown')\n",
    ")\n",
    "\n",
    "# Convertir las fechas de las pólizas al formato adecuado\n",
    "data_cleaned = data_cleaned.withColumn(\n",
    "    'policy_start_date', to_date(col('policy_start_date'), 'yyyy-MM-dd')\n",
    ").withColumn(\n",
    "    'policy_end_date', to_date(col('policy_end_date'), 'yyyy-MM-dd')\n",
    ")\n",
    "\n",
    "# Filtrar solo las columnas necesarias y crear el DataFrame 'data_policy'\n",
    "data_policy = data_cleaned.select(\n",
    "    col('policy_number'),\n",
    "    col('policy_start_date'),\n",
    "    col('policy_end_date'),\n",
    "    col('policy_type'),\n",
    "    col('insurance_company')\n",
    ")\n",
    "\n",
    "# Conectar a SQL Server utilizando pymssql\n",
    "\n",
    "conn = pymssql.connect(\n",
    "    server=\"LAPTOdP-O07NdV287\",  # Dirección del servidor\n",
    "    user=\"dddddd\",         # Nombre de usuario\n",
    "    password=\"dddd\",     # Contraseña\n",
    "    database=\"monokera\"        # Nombre de la base de datos\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Función para cargar un DataFrame a la base de datos\n",
    "def load_to_sql(df, table_name):\n",
    "    for row in df.collect():\n",
    "        query = f\"INSERT INTO {table_name} ({', '.join(df.columns)}) VALUES ({', '.join(['%s'] * len(df.columns))})\"\n",
    "        cursor.execute(query, tuple(row))\n",
    "    conn.commit()\n",
    "\n",
    "# Cargar DataFrames a las tablas de SQL Server (asegurándote de que las tablas ya están creadas)\n",
    "load_to_sql(data_policy, 'policy')\n",
    "load_to_sql(data_agents_filtered, 'agents')\n",
    "load_to_sql(data_premium, 'premium')\n",
    "load_to_sql(data_payments_filtered, 'payments')\n",
    "load_to_sql(data_insured, 'insured')\n",
    "load_to_sql(data_claim, 'claims')\n",
    "\n",
    "# Cerrar la conexión\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Datos cargados correctamente en SQL Server.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:  ['id', 'first_name', 'last_name', 'email', 'gender', 'policy_number', 'insured_name', 'insured_gender', 'insured_age', 'insured_address', 'insured_city', 'insured_state', 'insured_postal_code', 'insured_country', 'policy_start_date', 'policy_end_date', 'premium_amount', 'deductible_amount', 'coverage_limit', 'policy_type', 'insurance_company', 'agent_name', 'agent_email', 'agent_phone', 'claim_status', 'claim_date', 'claim_amount', 'claim_description', 'payment_status', 'payment_date', 'payment_amount', 'payment_method']\n",
      "Datos cargados correctamente en la tabla policy.\n",
      "Error al cargar datos en la tabla agents: (207, b\"Invalid column name 'id'.DB-Lib error message 20018, severity 16:\\nGeneral SQL Server error: Check messages from the SQL Server\\n\")\n",
      "Error al cargar datos en la tabla premium: (207, b\"Invalid column name 'id'.DB-Lib error message 20018, severity 16:\\nGeneral SQL Server error: Check messages from the SQL Server\\n\")\n",
      "Error al cargar datos en la tabla payments: (207, b\"Invalid column name 'id'.DB-Lib error message 20018, severity 16:\\nGeneral SQL Server error: Check messages from the SQL Server\\n\")\n",
      "Error al cargar datos en la tabla insured: (207, b\"Invalid column name 'id'.DB-Lib error message 20018, severity 16:\\nGeneral SQL Server error: Check messages from the SQL Server\\n\")\n",
      "Error al cargar datos en la tabla claims: (207, b\"Invalid column name 'id'.DB-Lib error message 20018, severity 16:\\nGeneral SQL Server error: Check messages from the SQL Server\\n\")\n",
      "Datos cargados correctamente en SQL Server.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, regexp_extract\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import functions as F  # Importar funciones como F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType\n",
    "import pymssql\n",
    "\n",
    "# Inicializa la sesión de Spark\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"DataCleaning\").getOrCreate()\n",
    "\n",
    "# Ruta del archivo cargado\n",
    "file_path = \"C:\\\\Users\\\\Usuario\\\\Documents\\\\WILLIAM\\\\DOCUMENTOS\\\\PERSONALES\\\\MONOKERA\\\\MOCK_DATA.csv\"\n",
    "\n",
    "# Leer el archivo CSV y asegurarse de que tiene encabezado y tipo de datos inferido\n",
    "data_3 = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Verificar los nombres de las columnas\n",
    "print(\"Column names: \", data_3.columns)\n",
    "\n",
    "# Limpiar la columna 'id' para extraer solo los números (si están en el formato esperado)\n",
    "data_3 = data_3.withColumn('id', regexp_extract(col('id'), r'\\d+', 0))\n",
    "\n",
    "# Filtrar filas donde 'id' no es nulo y contiene solo números\n",
    "data_3_cleaned = data_3.filter(col('id').rlike('^\\d+$'))\n",
    "\n",
    "# Verificar si las columnas de fecha existen y convertirlas al formato correcto\n",
    "date_columns = [\"claim_date\", \"payment_date\", \"policy_start_date\", \"policy_end_date\"]\n",
    "for col_name in date_columns:\n",
    "    if col_name in data_3_cleaned.columns:\n",
    "        data_3_cleaned = data_3_cleaned.withColumn(col_name, to_date(col(col_name), \"M/d/yyyy\"))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada\")\n",
    "\n",
    "# Manejo de valores nulos\n",
    "# Rellenar valores nulos con valores predeterminados\n",
    "data_cleaned = data_3_cleaned.fillna({\n",
    "    \"insured_age\": 0,\n",
    "    \"insured_gender\": \"Unknown\",\n",
    "    \"claim_status\": \"Unknown\",\n",
    "    \"payment_status\": \"Unknown\",\n",
    "    \"payment_method\": \"Unknown\"\n",
    "})\n",
    "\n",
    "# Convertir columnas numéricas relevantes a tipos adecuados\n",
    "numeric_columns = [\"claim_amount\", \"payment_amount\"]\n",
    "for col_name in numeric_columns:\n",
    "    if col_name in data_cleaned.columns:\n",
    "        data_cleaned = data_cleaned.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada para conversión a tipo numérico\")\n",
    "\n",
    "# Filtrar y seleccionar las columnas necesarias para 'data_claim'\n",
    "data_claim = data_cleaned.filter(F.col('claim_date').isNotNull()) \\\n",
    "    .select(\n",
    "        F.col('policy_number').alias('id'),\n",
    "        'claim_status',\n",
    "        'claim_date',\n",
    "        'claim_amount',\n",
    "        'claim_description'\n",
    "    )\n",
    "\n",
    "# Agrupar por 'policy_number' y obtener los primeros valores de 'insured_name', 'insured_gender', 'insured_age', etc.\n",
    "data_insured = data_cleaned.groupBy(\"policy_number\").agg(\n",
    "    F.first('insured_name').alias('insured_name'),\n",
    "    F.first('insured_gender').alias('insured_gender'),\n",
    "    F.first('insured_age').alias('insured_age'),\n",
    "    F.first('insured_address').alias('insured_address'),\n",
    "    F.first('insured_city').alias('insured_city'),\n",
    "    F.first('insured_state').alias('insured_state'),\n",
    "    F.first('insured_postal_code').alias('insured_postal_code'),\n",
    "    F.first('insured_country').alias('insured_country')\n",
    ").withColumnRenamed(\"policy_number\", \"id\")\n",
    "\n",
    "# Filtrar las columnas que contienen \"payment\" en su nombre\n",
    "payment_columns = [col_name for col_name in data_cleaned.columns if \"payment\" in col_name]\n",
    "\n",
    "# Crear el DataFrame con las columnas de payment y policy_number como id\n",
    "data_payments = data_cleaned.select(\n",
    "    F.col('policy_number').alias('id'),\n",
    "    *payment_columns\n",
    ")\n",
    "\n",
    "# Filtrar filas donde payment_status sea diferente de 'Unknown'\n",
    "data_payments_filtered = data_payments.filter(F.col('payment_status') != 'Unknown')\n",
    "\n",
    "# Agrupar por 'policy_number' y obtener los primeros valores de 'premium_amount', 'deductible_amount', etc.\n",
    "data_premium = data_cleaned.groupBy(\"policy_number\").agg(\n",
    "    F.first('premium_amount').alias('premium_amount'),\n",
    "    F.first('deductible_amount').alias('deductible_amount'),\n",
    "    F.first('coverage_limit').alias('coverage_limit')  \n",
    ").withColumnRenamed(\"policy_number\", \"id\")\n",
    "\n",
    "# Agrupar por 'policy_number' y obtener los primeros valores de 'agent_name', 'agent_email', 'agent_phone'\n",
    "data_agents = data_cleaned.groupBy(\"policy_number\").agg(\n",
    "    F.first('agent_name').alias('agent_name'),\n",
    "    F.first('agent_email').alias('agent_email'),\n",
    "    F.first('agent_phone').alias('agent_phone')\n",
    ").withColumnRenamed(\"policy_number\", \"id\")\n",
    "\n",
    "# Filtrar para eliminar agentes con 'agent_name' nulo o 'Unknown'\n",
    "data_agents_filtered = data_agents.filter(\n",
    "    (F.col('agent_name').isNotNull()) & (F.col('agent_name') != 'Unknown')\n",
    ")\n",
    "\n",
    "# Convertir las fechas de las pólizas al formato adecuado\n",
    "data_cleaned = data_cleaned.withColumn(\n",
    "    'policy_start_date', to_date(col('policy_start_date'), 'yyyy-MM-dd')\n",
    ").withColumn(\n",
    "    'policy_end_date', to_date(col('policy_end_date'), 'yyyy-MM-dd')\n",
    ")\n",
    "\n",
    "# Manejo de valores nulos en fechas de pólizas (rellenar con una fecha predeterminada si es necesario)\n",
    "data_cleaned = data_cleaned.fillna({\n",
    "    \"policy_start_date\": \"1900-01-01\",\n",
    "    \"policy_end_date\": \"1900-01-01\"\n",
    "})\n",
    "\n",
    "# Filtrar solo las columnas necesarias y crear el DataFrame 'data_policy'\n",
    "data_policy = data_cleaned.select(\n",
    "    col('policy_number'),\n",
    "    col('policy_start_date'),\n",
    "    col('policy_end_date'),\n",
    "    col('policy_type'),\n",
    "    col('insurance_company')\n",
    ")\n",
    "\n",
    "# Conectar a SQL Server utilizando pymssql\n",
    "conn = pymssql.connect(\n",
    "    server=\"LAPTOP-O07NVsss287\",  # Dirección del servidor\n",
    "    user=\"\",         # Nombre de usuario\n",
    "    password=\"\",     # Contraseña\n",
    "    database=\"monokera\"        # Nombre de la base de datos\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Función para cargar un DataFrame a la base de datos\n",
    "def load_to_sql(df, table_name):\n",
    "    try:\n",
    "        for row in df.collect():\n",
    "            query = f\"INSERT INTO {table_name} ({', '.join(df.columns)}) VALUES ({', '.join(['%s'] * len(df.columns))})\"\n",
    "            cursor.execute(query, tuple(row))\n",
    "        conn.commit()\n",
    "        print(f\"Datos cargados correctamente en la tabla {table_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar datos en la tabla {table_name}: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "# Cargar DataFrames a las tablas de SQL Server (asegurándote de que las tablas ya están creadas)\n",
    "load_to_sql(data_policy, 'policy')\n",
    "load_to_sql(data_agents_filtered, 'agents')\n",
    "load_to_sql(data_premium, 'premium')\n",
    "load_to_sql(data_payments_filtered, 'payments')\n",
    "load_to_sql(data_insured, 'insured')\n",
    "load_to_sql(data_claim, 'claims')\n",
    "\n",
    "# Cerrar la conexión\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Datos cargados correctamente en SQL Server.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:  ['id', 'first_name', 'last_name', 'email', 'gender', 'policy_number', 'insured_name', 'insured_gender', 'insured_age', 'insured_address', 'insured_city', 'insured_state', 'insured_postal_code', 'insured_country', 'policy_start_date', 'policy_end_date', 'premium_amount', 'deductible_amount', 'coverage_limit', 'policy_type', 'insurance_company', 'agent_name', 'agent_email', 'agent_phone', 'claim_status', 'claim_date', 'claim_amount', 'claim_description', 'payment_status', 'payment_date', 'payment_amount', 'payment_method']\n",
      "Datos cargados correctamente en la tabla policy.\n",
      "Datos cargados correctamente en la tabla agents.\n",
      "Datos cargados correctamente en la tabla premium.\n",
      "Datos cargados correctamente en la tabla payments.\n",
      "Datos cargados correctamente en la tabla insured.\n",
      "Datos cargados correctamente en la tabla claims.\n",
      "Datos cargados correctamente en SQL Server.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, regexp_extract\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import functions as F  # Importar funciones como F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType\n",
    "import pymssql\n",
    "\n",
    "# Inicializa la sesión de Spark\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"DataCleaning\").getOrCreate()\n",
    "\n",
    "# Ruta del archivo cargado\n",
    "file_path = \"C:\\\\Users\\\\Usuario\\\\Documents\\\\WILLIAM\\\\DOCUMENTOS\\\\PERSONALES\\\\MONOKERA\\\\MOCK_DATA.csv\"\n",
    "\n",
    "# Leer el archivo CSV y asegurarse de que tiene encabezado y tipo de datos inferido\n",
    "data_3 = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Verificar los nombres de las columnas\n",
    "print(\"Column names: \", data_3.columns)\n",
    "\n",
    "# Limpiar la columna 'id' para extraer solo los números (si están en el formato esperado)\n",
    "data_3 = data_3.withColumn('id', regexp_extract(col('id'), r'\\d+', 0))\n",
    "\n",
    "# Filtrar filas donde 'id' no es nulo y contiene solo números\n",
    "data_3_cleaned = data_3.filter(col('id').rlike('^\\d+$'))\n",
    "\n",
    "# Verificar si las columnas de fecha existen y convertirlas al formato correcto\n",
    "date_columns = [\"claim_date\", \"payment_date\", \"policy_start_date\", \"policy_end_date\"]\n",
    "for col_name in date_columns:\n",
    "    if col_name in data_3_cleaned.columns:\n",
    "        data_3_cleaned = data_3_cleaned.withColumn(col_name, to_date(col(col_name), \"M/d/yyyy\"))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada\")\n",
    "\n",
    "# Manejo de valores nulos\n",
    "# Rellenar valores nulos con valores predeterminados\n",
    "data_cleaned = data_3_cleaned.fillna({\n",
    "    \"insured_age\": 0,\n",
    "    \"insured_gender\": \"Unknown\",\n",
    "    \"claim_status\": \"Unknown\",\n",
    "    \"payment_status\": \"Unknown\",\n",
    "    \"payment_method\": \"Unknown\"\n",
    "})\n",
    "\n",
    "# Convertir columnas numéricas relevantes a tipos adecuados\n",
    "numeric_columns = [\"claim_amount\", \"payment_amount\"]\n",
    "for col_name in numeric_columns:\n",
    "    if col_name in data_cleaned.columns:\n",
    "        data_cleaned = data_cleaned.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "    else:\n",
    "        print(f\"Columna '{col_name}' no encontrada para conversión a tipo numérico\")\n",
    "\n",
    "# Filtrar y seleccionar las columnas necesarias para 'data_claim'\n",
    "data_claim = data_cleaned.filter(F.col('claim_date').isNotNull()) \\\n",
    "    .select(\n",
    "        F.col('policy_number').alias('id'),\n",
    "        'claim_status',\n",
    "        'claim_date',\n",
    "        'claim_amount',\n",
    "        'claim_description'\n",
    "    )\n",
    "\n",
    "# Agrupar por 'policy_number' y obtener los primeros valores de 'insured_name', 'insured_gender', 'insured_age', etc.\n",
    "data_insured = data_cleaned.groupBy(\"policy_number\").agg(\n",
    "    F.first('insured_name').alias('insured_name'),\n",
    "    F.first('insured_gender').alias('insured_gender'),\n",
    "    F.first('insured_age').alias('insured_age'),\n",
    "    F.first('insured_address').alias('insured_address'),\n",
    "    F.first('insured_city').alias('insured_city'),\n",
    "    F.first('insured_state').alias('insured_state'),\n",
    "    F.first('insured_postal_code').alias('insured_postal_code'),\n",
    "    F.first('insured_country').alias('insured_country')\n",
    ").withColumnRenamed(\"policy_number\", \"id\")\n",
    "\n",
    "# Filtrar las columnas que contienen \"payment\" en su nombre\n",
    "payment_columns = [col_name for col_name in data_cleaned.columns if \"payment\" in col_name]\n",
    "\n",
    "# Crear el DataFrame con las columnas de payment y policy_number como id\n",
    "data_payments = data_cleaned.select(\n",
    "    F.col('policy_number').alias('id'),\n",
    "    *payment_columns\n",
    ")\n",
    "\n",
    "# Filtrar filas donde payment_status sea diferente de 'Unknown'\n",
    "data_payments_filtered = data_payments.filter(F.col('payment_status') != 'Unknown')\n",
    "\n",
    "# Agrupar por 'policy_number' y obtener los primeros valores de 'premium_amount', 'deductible_amount', etc.\n",
    "data_premium = data_cleaned.groupBy(\"policy_number\").agg(\n",
    "    F.first('premium_amount').alias('premium_amount'),\n",
    "    F.first('deductible_amount').alias('deductible_amount'),\n",
    "    F.first('coverage_limit').alias('coverage_limit')  \n",
    ").withColumnRenamed(\"policy_number\", \"id\")\n",
    "\n",
    "# Agrupar por 'policy_number' y obtener los primeros valores de 'agent_name', 'agent_email', 'agent_phone'\n",
    "data_agents = data_cleaned.groupBy(\"policy_number\").agg(\n",
    "    F.first('agent_name').alias('agent_name'),\n",
    "    F.first('agent_email').alias('agent_email'),\n",
    "    F.first('agent_phone').alias('agent_phone')\n",
    ").withColumnRenamed(\"policy_number\", \"id\")\n",
    "\n",
    "# Filtrar para eliminar agentes con 'agent_name' nulo o 'Unknown'\n",
    "data_agents_filtered = data_agents.filter(\n",
    "    (F.col('agent_name').isNotNull()) & (F.col('agent_name') != 'Unknown')\n",
    ")\n",
    "\n",
    "# Convertir las fechas de las pólizas al formato adecuado\n",
    "data_cleaned = data_cleaned.withColumn(\n",
    "    'policy_start_date', to_date(col('policy_start_date'), 'yyyy-MM-dd')\n",
    ").withColumn(\n",
    "    'policy_end_date', to_date(col('policy_end_date'), 'yyyy-MM-dd')\n",
    ")\n",
    "\n",
    "# Manejo de valores nulos en fechas de pólizas (rellenar con una fecha predeterminada si es necesario)\n",
    "data_cleaned = data_cleaned.fillna({\n",
    "    \"policy_start_date\": \"1900-01-01\",\n",
    "    \"policy_end_date\": \"1900-01-01\"\n",
    "})\n",
    "\n",
    "# Filtrar solo las columnas necesarias y crear el DataFrame 'data_policy'\n",
    "data_policy = data_cleaned.select(\n",
    "    col('policy_number'),\n",
    "    col('policy_start_date'),\n",
    "    col('policy_end_date'),\n",
    "    col('policy_type'),\n",
    "    col('insurance_company')\n",
    ")\n",
    "\n",
    "data_agents_filtered = data_agents_filtered.withColumnRenamed(\"id\", \"policy_number\")\n",
    "data_claim = data_claim.withColumnRenamed(\"id\", \"policy_number\")\n",
    "data_insured = data_insured.withColumnRenamed(\"id\", \"policy_number\")\n",
    "data_payments_filtered = data_payments_filtered.withColumnRenamed(\"id\", \"policy_number\")\n",
    "data_premium = data_premium.withColumnRenamed(\"id\", \"policy_number\")\n",
    "data_policy = data_policy.withColumnRenamed(\"id\", \"policy_number\")\n",
    "\n",
    "# Conectar a SQL Server utilizando pymssql\n",
    "conn = pymssql.connect(\n",
    "    server=\"LAPTOP-O07sssNV287\",  # Dirección del servidor\n",
    "    user=\"\",         # Nombre de usuario\n",
    "    password=\"\",     # Contraseña\n",
    "    database=\"monokera\"        # Nombre de la base de datos\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Función para cargar un DataFrame a la base de datos\n",
    "def load_to_sql(df, table_name):\n",
    "    try:\n",
    "        for row in df.collect():\n",
    "            query = f\"INSERT INTO {table_name} ({', '.join(df.columns)}) VALUES ({', '.join(['%s'] * len(df.columns))})\"\n",
    "            cursor.execute(query, tuple(row))\n",
    "        conn.commit()\n",
    "        print(f\"Datos cargados correctamente en la tabla {table_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar datos en la tabla {table_name}: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "# Cargar DataFrames a las tablas de SQL Server (asegurándote de que las tablas ya están creadas)\n",
    "load_to_sql(data_policy, 'policy')\n",
    "load_to_sql(data_agents_filtered, 'agents')\n",
    "load_to_sql(data_premium, 'premium')\n",
    "load_to_sql(data_payments_filtered, 'payments')\n",
    "load_to_sql(data_insured, 'insured')\n",
    "load_to_sql(data_claim, 'claims')\n",
    "\n",
    "# Cerrar la conexión\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Datos cargados correctamente en SQL Server.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
